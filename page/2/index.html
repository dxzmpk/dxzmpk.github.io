<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E8%A7%86%E5%8A%9B%E8%A1%A8%20(1).svg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E8%A7%86%E5%8A%9B%E8%A1%A8.svg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dxzmpk.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Learning in Harbin Institute of Technology">
<meta property="og:type" content="website">
<meta property="og:title" content="董雄写字的地方">
<meta property="og:url" content="https://dxzmpk.github.io/page/2/">
<meta property="og:site_name" content="董雄写字的地方">
<meta property="og:description" content="Learning in Harbin Institute of Technology">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="董雄">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content=" cs">
<meta property="article:tag" content=" hit">
<meta property="article:tag" content=" transformers">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://dxzmpk.github.io/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>董雄写字的地方</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">董雄写字的地方</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">endless hard working</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/05/08/%E4%B8%89%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%BC%80%E5%90%AF%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E7%9F%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="Learning in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄写字的地方">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/08/%E4%B8%89%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%BC%80%E5%90%AF%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E7%9F%A5/" class="post-title-link" itemprop="url">三行代码开启模型训练通知</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-08 12:12:05" itemprop="dateCreated datePublished" datetime="2020-05-08T12:12:05+08:00">2020-05-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-14 21:15:05" itemprop="dateModified" datetime="2020-05-14T21:15:05+08:00">2020-05-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>如果你经常在大量数据上训练模型的话，那么这样一个工具肯定很合你的胃口。这个项目叫做<a href="https://github.com/huggingface/knockknock" target="_blank" rel="noopener">knockknock</a>，它的功能只有一个：通知你训练结束了，并且附带训练的结果。</p>
<p>当前支持邮件、短信、微信群、钉钉群等通知方式，只需要三行代码，就能实现功能。</p>
<h2 id="效果展示"><a href="#效果展示" class="headerlink" title="效果展示"></a>效果展示</h2><p>为了让你有看下去的动力，先展示一下最终的成果(钉钉)：</p>
<p><img src="/images/%E4%B8%89%E8%A1%8C%E4%BB%A3%E7%A0%81%E5%BC%80%E5%90%AF%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9A%E7%9F%A5/share.jpg" alt=""></p>
<h2 id="knockknock在钉钉群中的配置方式"><a href="#knockknock在钉钉群中的配置方式" class="headerlink" title="knockknock在钉钉群中的配置方式"></a><a href="https://github.com/huggingface/knockknock" target="_blank" rel="noopener">knockknock</a>在钉钉群中的配置方式</h2><p>之所以选择使用钉钉，是因为它的通知声音比较好听🤭。配置过程：</p>
<ol>
<li><p>建立钉钉群（最好是在电脑）</p>
</li>
<li><p>添加机器人</p>
<p>2.1 群设置$\rightarrow $智能群助手$\rightarrow $添加机器人</p>
<p>2.2 添加自定义机器人</p>
<p>2.3 自定义名字和头像</p>
<p>2.4 选择一种加密方式，推荐选择<code>加签</code></p>
<p>2.5 记录好<code>机器人的url</code>和加签生成的<code>密钥</code></p>
</li>
<li><p>在notebook或者python虚拟环境中安装knockknock</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">!pip install knockknock(notebook)</span><br><span class="line"><span class="keyword">or</span></span><br><span class="line">pip install knockknock(虚拟环境)</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入dingtalk_sender</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> knockknock <span class="keyword">import</span> dingtalk_sender</span><br></pre></td></tr></table></figure>
</li>
<li><p>在要跑的类上添加以下代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">webhook_url = <span class="string">"https://oapi.dingtalk.com/robot/send?access_token=..."</span></span><br><span class="line"><span class="meta">@dingtalk_sender(webhook_url=webhook_url, secret="加签生成的密钥", keywords=["随便填"])</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_your_nicest_model</span><span class="params">(your_nicest_parameters)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    time.sleep(<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'loss'</span>: <span class="number">0.9</span>&#125; <span class="comment"># Optional return value</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>运行train_your_nicest_model，得到结果</p>
<p><img src="https://i.loli.net/2020/05/14/vzGJgxiVu5j2Scb.jpg" alt="微信图片_20200514211147"></p>
</li>
</ol>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>本文主要参考自官方<a href="https://github.com/huggingface/knockknock" target="_blank" rel="noopener">github仓库</a></p>
<p>以及<a href="https://ding-doc.dingtalk.com/doc#/serverapi2/qf2nxq" target="_blank" rel="noopener">阿里钉钉开发者平台</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/29/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="Learning in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄写字的地方">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/29/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/" class="post-title-link" itemprop="url">Bert系列伴生的新分词器</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-29 09:31:52" itemprop="dateCreated datePublished" datetime="2020-04-29T09:31:52+08:00">2020-04-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-04 09:53:09" itemprop="dateModified" datetime="2020-05-04T09:53:09+08:00">2020-05-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>这篇文章将对Bert等模型使用的分词技术进行介绍。同时会涉及这些分词器在huggingface <a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">tokenizers</a>库中的使用。理解这些分词器的原理，对于灵活使用transformers库中的不同模型非常重要。除此之外，我们还能将这些分词器用于其他任务中，如果有必要的话，我们还能自己训练分词器。</p>
<h2 id="分词器是做什么的？"><a href="#分词器是做什么的？" class="headerlink" title="分词器是做什么的？"></a>分词器是做什么的？</h2><p><strong>机器无法理解文本。</strong>当我们将句子序列送入模型时，模型仅仅能看到一串字节，它无法知道一个词从哪里开始，到哪里结束，所以也不知道一个词是怎么组成的。</p>
<p>​    所以，为了帮助机器理解文本，我们需要</p>
<ol>
<li>将文本分成一个个小片段</li>
<li>然后将这些片段表示为一个向量作为模型的输入</li>
<li>同时，我们需要将一个个小片段（token) 表示为向量，作为词嵌入矩阵， 通过在语料库上训练来优化token的表示，使其蕴含更多有用的信息，用于之后的任务。</li>
</ol>
<h2 id="古典分词方法"><a href="#古典分词方法" class="headerlink" title="古典分词方法"></a>古典分词方法</h2><p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/tokenize.png" alt="tokenize"></p>
<h3 id="基于空格的分词方法"><a href="#基于空格的分词方法" class="headerlink" title="基于空格的分词方法"></a>基于空格的分词方法</h3><p>一个句子，使用不同的规则，将有许多种不同的分词结果。我们之前常用的分词方法将空格作为分词的边界。也就是图中的第三种方法。但是，这种方法存在问题，即只有在训练语料中出现的token才能被训练器学习到，而那些没有出现的token将会被<code>&lt;UNK&gt;</code>等特殊标记代替，这样将影响模型的表现。如果我们将词典做得足够大，使其能容纳所有的单词。那么词典将非常庞大，产生很大的开销。同时对于出现次数很少的词，学习其token的向量表示也非常困难。除去这些原因，有很多语言不用空格进行分词，也就无法使用基于空格分词的方法。综上，我们需要新的分词方法来解决这些问题。</p>
<h3 id="基于字母的分词方法"><a href="#基于字母的分词方法" class="headerlink" title="基于字母的分词方法"></a>基于字母的分词方法</h3><p>简单来说，就是将每个字符看作一个词。</p>
<p><strong>优点</strong>： 不用担心未知词汇，可以为每一个单词生成词嵌入向量表示。</p>
<p><strong>缺点</strong>：</p>
<ul>
<li>字母本身就没有任何的内在含义，得到的词嵌入向量缺乏含义。</li>
<li>计算复杂度提升（字母的数目远大于token的数目）</li>
<li>输出序列的长度将变大，对于Bert、CNN等限制最大长度的模型将很容易达到最大值。</li>
</ul>
<h2 id="基于子词的分词方法（Subword-Tokenization）"><a href="#基于子词的分词方法（Subword-Tokenization）" class="headerlink" title="基于子词的分词方法（Subword Tokenization）"></a>基于子词的分词方法（Subword Tokenization）</h2><p>为了改进分词方法，在<code>&lt;UNK&gt;</code>数目和词向量含义丰富性之间达到平衡，提出了Subword Tokenization方法。这种方法的目的是通过一个有限的单词列表来解决所有单词的分词问题，同时将结果中token的数目降到最低。例如，可以用更小的词片段来组成更大的词：</p>
<p>“<strong><em>unfortunately</em></strong>” = “<strong><em>un</em></strong>” + “<strong><em>for</em></strong>” + “<strong><em>tun</em></strong>” + “<strong><em>ate</em></strong>” + “<strong><em>ly</em></strong>”。</p>
<p>接下来，将介绍几种不同的Subword Tokenization方法。</p>
<h3 id="Byte-Pair-Encoding-BPE-字节对编码"><a href="#Byte-Pair-Encoding-BPE-字节对编码" class="headerlink" title="Byte Pair Encoding (BPE) 字节对编码"></a>Byte Pair Encoding (BPE) 字节对编码</h3><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>字节对编码最早是在信号压缩领域提出的，后来被应用于分词任务中。在信号压缩领域中BPE过程可视化如下：</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/1_x1Y_n3sXGygUPSdfXTm9pQ.gif" alt="1_x1Y_n3sXGygUPSdfXTm9pQ"></p>
<p>接下来重点介绍将BPE应用于分词任务的流程：</p>
<h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><ol>
<li>根据语料库建立一个词典，词典中仅包含单个字符，如英文中就是a-z</li>
<li>统计语料库中出现次数最多的字符对（词典中两项的组合），然后将字符对加入到词典中</li>
<li>重复步骤2直到到达规定的步骤数目或者词典尺寸缩小到了指定的值。</li>
</ol>
<h3 id="BPE的优点"><a href="#BPE的优点" class="headerlink" title="BPE的优点"></a>BPE的优点</h3><p>可以很有效地平衡词典尺寸和编码步骤数(将句子编码所需要的token数量)</p>
<h3 id="BPE存在的缺点："><a href="#BPE存在的缺点：" class="headerlink" title="BPE存在的缺点："></a><strong>BPE</strong>存在的缺点：</h3><p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/image-20200429104839759.png" alt="image-20200429104839759"></p>
<ul>
<li>对于同一个句子, 例如Hello world，如图所示，可能会有不同的Subword序列。不同的Subword序列会产生完全不同的id序列表示，这种歧义可能在解码阶段无法解决。在翻译任务中，不同的id序列可能翻译出不同的句子，这显然是错误的。</li>
<li>在训练任务中，如果能对不同的Subword进行训练的话，将增加模型的健壮性，能够容忍更多的噪声，而BPE的贪心算法无法对随机分布进行学习。</li>
</ul>
<h3 id="Unigram-Based-Tokenization"><a href="#Unigram-Based-Tokenization" class="headerlink" title="Unigram Based Tokenization"></a>Unigram Based Tokenization</h3><h3 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h3><p>分词中的Unigram模型是<strong>Kudo.</strong>在论文<strong>“Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”</strong>中提出的。当时主要是为了解决机器翻译中分词的问题。作者使用一种叫做<code>marginalized likelihood</code>的方法来建模翻译问题，考虑到不同分词结果对最终翻译结果的影响，引入了分词概率$P(\vec{x}|X)$来表示$X$最终分词为$\vec{x}$的概率(X为原始的句子, $\vec{x}$为分词的结果$\vec{x} = (x_1, . . . , x_M) $，由多个subword组成)。传统的BPE算法无法对这个概率进行建模，因此作者使用了Unigram语言模型来达到这样的目的。</p>
<h3 id="方法执行过程"><a href="#方法执行过程" class="headerlink" title="方法执行过程"></a>方法执行过程</h3><p><strong>假设</strong>：根据unigram的假设，每个字词的出现是独立的。所以</p>
<script type="math/tex; mode=display">
P(\vec{x}) = \prod_{i=1}^{M}p(x_i)</script><p>这里的$x_i$是从预先定义好的词典$V$中取得的，所以，最有可能的分词方式就可以这样表示：</p>
<script type="math/tex; mode=display">
x^* =\underset{x\in S(X)}{arg\;max}\;P(\vec{x})</script><p>这里$S(X)$是句子$X$不同的分词结果集合。$x^*$可以通过维特比算法得到。</p>
<p>如果已知词典$V$, 我们可以通过EM算法来估计$p(x_i)$，其中M步最大化的对象是以下似然函数（原谅我这里偷懒直接使用图片）：</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/image-20200501185312612.png" alt="image-20200501185312612"></p>
<p>$|D|$是语料库中语料数量。</p>
<p><strong>我是这样理解这个似然函数的：</strong>将语料库中所有句子的所有分词组合形成的概率相加。</p>
<p>初始时，我们连词典$V$都没有，作者通过不断执行以下步骤来构造合适的词典以及分词概率：</p>
<ol>
<li><p>从头构建一个相当大的种子词典。</p>
</li>
<li><p>重复以下步骤，知道字典尺寸$|V|$减小到期望值：</p>
<ul>
<li><p>固定词典，通过EM算法优化$p(x)$</p>
</li>
<li><p>为每一个子词计算$loss_i$，loss代表如果将某个词去掉，上述似然函数值会减少多少。根据loss排序，保留loss最高的$\eta$个子词。注意：保留所有的单字符，从而避免OOV情况。</p>
<p><strong>我是这样理解loss的：</strong>若某个子词经常以很高的频率出现在很多句子的分词结果中，那么其损失将会很大，所以要保留这样的子词。</p>
</li>
</ul>
</li>
</ol>
<h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><ol>
<li>使用的训练算法可以利用所有可能的分词结果，这是通过data sampling算法实现的。</li>
<li>提出一种基于语言模型的分词算法，这种语言模型可以给多种分词结果赋予概率，从而可以学得其中的噪声。</li>
</ol>
<h2 id="将基于子词的分词方法应用到实际中"><a href="#将基于子词的分词方法应用到实际中" class="headerlink" title="将基于子词的分词方法应用到实际中"></a>将基于子词的分词方法应用到实际中</h2><h3 id="Bert中的WordPiece分词器"><a href="#Bert中的WordPiece分词器" class="headerlink" title="Bert中的WordPiece分词器"></a>Bert中的WordPiece分词器</h3><p>WordPiece是随着Bert论文的出现被提出的。在整体步骤上，WordPiece方法和BPE是相同的。即也是自低向上地构建词典。区别是BPE在每次合并的时候都选择出现次数最高的字符对，而WordPiece使用的是类似于Unigram的方法，即通过语言模型来得到合并两个单词可能造成的影响，然后选择使得似然函数提升最大的字符对。这个提升是通过结合后的字符对减去结合前的字符对之和得到的。也就是说，判断“de”相较于“d”+”e”是否更适合出现。</p>
<p>三种分词器的关系如下：(图自<a href="https://blog.floydhub.com/tokenization-nlp/" target="_blank" rel="noopener">FloudHub Blog</a>)</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/subword-probabilistic-tokenization.png" alt="Frequency V probability approaches"></p>
<h3 id="SentencePiece库"><a href="#SentencePiece库" class="headerlink" title="SentencePiece库"></a>SentencePiece库</h3><p>SentencePiece是在“SentencePiece: A simple and language independent subword tokenizer<br>and detokenizer for Neural Text Processing”这篇文章中介绍的。其主要是为了解决不同语言分词规则需要特别定义的问题，比如下面这种情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Raw text: Hello world.</span><br><span class="line">Tokenized: [Hello] [world] [.]</span><br><span class="line">Decoded text: Hello world .</span><br></pre></td></tr></table></figure>
<p>将分词结果解码到原来的句子中时，会在不同的词之间添加空格，生成<code>Decoded text</code>所示的结果，这就是编码解码出现的歧义性，因此需要特别定义规则来实现互逆。还有一个例子是，在解码阶段，欧洲语言词之间要添加空格，而中文等语言则不应添加空格。对于这种区别，也需要单独定制规则，这些繁杂的规则维护起来非常困难，所以作者采用以下的方案来解决：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将所有的字符都转化成Unicode编码，空格用‘_’来代替，然后进行分词操作。这样空格也不需要特别定义规则了。然后在解码结束后，使用Python代码恢复即可：</span><br><span class="line">detok = ’’.join(tokens).replace(’_’, ’ ’)</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece库</a>主要由以下部分组成：</p>
<p><strong>“Normalizer, Trainer, Encoder,  Decoder”</strong></p>
<p>其中Normalizer用来对Unicode编码进行规范化，这里使用的算法是<code>NFKC</code>方法，同时也支持自定义规范化方法。Trainer则用来训练分词模型。Encoder是将句子变成编码，而Decoder是反向操作。他们之间存在以下函数关系：</p>
<script type="math/tex; mode=display">
Decode(Encode(Normalize(text))) = Normalize(text):</script><h3 id="Huggingface-tokenizers库的介绍和使用"><a href="#Huggingface-tokenizers库的介绍和使用" class="headerlink" title="Huggingface tokenizers库的介绍和使用"></a>Huggingface tokenizers库的介绍和使用</h3><p><a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">tokenizers</a>是集合了当前最常用的分词器集合，效率和易用性也是其关注的范畴。</p>
<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenizers provides ultra-fast implementations of most current tokenizers:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (ByteLevelBPETokenizer,</span><br><span class="line">                            CharBPETokenizer,</span><br><span class="line">                            SentencePieceBPETokenizer,</span><br><span class="line">                            BertWordPieceTokenizer)</span><br><span class="line"><span class="comment"># Ultra-fast =&gt; they can encode 1GB of text in ~20sec on a standard server's CPU</span></span><br><span class="line"><span class="comment"># Tokenizers can be easily instantiated from standard files</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertWordPieceTokenizer(<span class="string">"bert-base-uncased-vocab.txt"</span>, lowercase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.</span></span><br><span class="line"><span class="comment"># They also handle model's max input lengths as well as padding (to directly encode in padded batches)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = tokenizer.encode(<span class="string">"Hello, y'all! How are you 😁 ?"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.ids, output.tokens, output.offsets)</span><br><span class="line">[<span class="number">101</span>, <span class="number">7592</span>, <span class="number">1010</span>, <span class="number">1061</span>, <span class="number">1005</span>, <span class="number">2035</span>, <span class="number">999</span>, <span class="number">2129</span>, <span class="number">2024</span>, <span class="number">2017</span>, <span class="number">100</span>, <span class="number">1029</span>, <span class="number">102</span>]</span><br><span class="line">[<span class="string">'[CLS]'</span>, <span class="string">'hello'</span>, <span class="string">','</span>, <span class="string">'y'</span>, <span class="string">"'"</span>, <span class="string">'all'</span>, <span class="string">'!'</span>, <span class="string">'how'</span>, <span class="string">'are'</span>, <span class="string">'you'</span>, <span class="string">'[UNK]'</span>, <span class="string">'?'</span>, <span class="string">'[SEP]'</span>]</span><br><span class="line">[(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">5</span>), (<span class="number">5</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">8</span>), (<span class="number">8</span>, <span class="number">9</span>), (<span class="number">9</span>, <span class="number">12</span>), (<span class="number">12</span>, <span class="number">13</span>), (<span class="number">14</span>, <span class="number">17</span>), (<span class="number">18</span>, <span class="number">21</span>), (<span class="number">22</span>, <span class="number">25</span>), (<span class="number">26</span>, <span class="number">27</span>),(<span class="number">28</span>, <span class="number">29</span>), (<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line"><span class="comment"># Here is an example using the offsets mapping to retrieve the string corresponding to the 10th token:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.original_str[output.offsets[<span class="number">10</span>]]</span><br><span class="line"><span class="string">'😁'</span></span><br></pre></td></tr></table></figure>
<h3 id="自己训练分词器"><a href="#自己训练分词器" class="headerlink" title="自己训练分词器"></a>自己训练分词器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># You can also train a BPE/Byte-levelBPE/WordPiece vocabulary on your own files</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = ByteLevelBPETokenizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.train([<span class="string">"wiki.test.raw"</span>], vocab_size=<span class="number">20000</span>)</span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>] Tokenize words                 ████████████████████████████████████████   <span class="number">20993</span>/<span class="number">20993</span></span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>] Count pairs                    ████████████████████████████████████████   <span class="number">20993</span>/<span class="number">20993</span></span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">03</span>] Compute merges                 ████████████████████████████████████████   <span class="number">19375</span>/<span class="number">19375</span></span><br></pre></td></tr></table></figure>
<h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><p>这篇文章是在Floydhub的<a href="https://blog.floydhub.com/tokenization-nlp/" target="_blank" rel="noopener">一篇博客</a>基础上扩展的。还主要参考了Unigram的原论文，BPE的官方解释等。BPE的动态图来自于Toward data science的<a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" target="_blank" rel="noopener">有关博客</a>。除此之外，最后一章参考于tokenizers的<a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">官方仓库</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0notebook%E5%A4%9A%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%A0%B7%E6%9C%AC%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="Learning in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄写字的地方">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0notebook%E5%A4%9A%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%A0%B7%E6%9C%AC%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">深度学习Notebook多折交叉验证样本代码</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-27 11:45:20 / 修改时间：12:17:16" itemprop="dateCreated datePublished" datetime="2020-04-27T11:45:20+08:00">2020-04-27</time>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习notebook多折交叉验证样本代码"><a href="#深度学习notebook多折交叉验证样本代码" class="headerlink" title="深度学习notebook多折交叉验证样本代码"></a>深度学习notebook多折交叉验证样本代码</h1><p><strong>这份样本代码基于 @abhishek’s <a href="https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch" target="_blank" rel="noopener">BERT Base Uncased using PyTorch</a>在tweet情感词抽取比赛上的notebook</strong>, 如果决定使用代码，请为abhishek投上一个赞成票。</p>
<h1 id="导入需要用到的包"><a href="#导入需要用到的包" class="headerlink" title="导入需要用到的包"></a>导入需要用到的包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="comment"># tqdm用来记录notebook训练的进度条</span></span><br><span class="line"><span class="keyword">from</span> tqdm.autonotebook <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">import</span> tokenizers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_linear_schedule_with_warmup</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">config</span>:</span></span><br><span class="line">    MAX_LEN = <span class="number">128</span></span><br><span class="line">    TRAIN_BATCH_SIZE = <span class="number">64</span></span><br><span class="line">    VALID_BATCH_SIZE = <span class="number">16</span></span><br><span class="line">    EPOCHS = <span class="number">6</span></span><br><span class="line">    ROBERTA_PATH = <span class="string">"../input/roberta-base/"</span></span><br><span class="line">    MODEL_PATH = <span class="string">"pytorch_model.bin"</span></span><br><span class="line">    TRAINING_FILE = <span class="string">"../input/tweet-train-folds/train_folds.csv"</span></span><br><span class="line">    TOKENIZER = tokenizers.ByteLevelBPETokenizer(</span><br><span class="line">    vocab_file=<span class="string">f"<span class="subst">&#123;ROBERTA_PATH&#125;</span>/vocab.json"</span>, </span><br><span class="line">    merges_file=<span class="string">f"<span class="subst">&#123;ROBERTA_PATH&#125;</span>/merges.txt"</span>, </span><br><span class="line">    lowercase=<span class="literal">True</span>,</span><br><span class="line">    add_prefix_space=<span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h1 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span><span class="params">(tweet, selected_text, sentiment, tokenizer, max_len)</span>:</span></span><br><span class="line">    tweet = <span class="string">" "</span> + <span class="string">" "</span>.join(str(tweet).split())</span><br><span class="line">    selected_text = <span class="string">" "</span> + <span class="string">" "</span>.join(str(selected_text).split())</span><br><span class="line"></span><br><span class="line">    len_st = len(selected_text) - <span class="number">1</span></span><br><span class="line">    idx0 = <span class="literal">None</span></span><br><span class="line">    idx1 = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> (i <span class="keyword">for</span> i, e <span class="keyword">in</span> enumerate(tweet) <span class="keyword">if</span> e == selected_text[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">" "</span> + tweet[ind: ind+len_st] == selected_text:</span><br><span class="line">            idx0 = ind</span><br><span class="line">            idx1 = ind + len_st - <span class="number">1</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    char_targets = [<span class="number">0</span>] * len(tweet)</span><br><span class="line">    <span class="keyword">if</span> idx0 != <span class="literal">None</span> <span class="keyword">and</span> idx1 != <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> ct <span class="keyword">in</span> range(idx0, idx1 + <span class="number">1</span>):</span><br><span class="line">            char_targets[ct] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    tok_tweet = tokenizer.encode(tweet)</span><br><span class="line">    input_ids_orig = tok_tweet.ids</span><br><span class="line">    tweet_offsets = tok_tweet.offsets</span><br><span class="line">    </span><br><span class="line">    target_idx = []</span><br><span class="line">    <span class="keyword">for</span> j, (offset1, offset2) <span class="keyword">in</span> enumerate(tweet_offsets):</span><br><span class="line">        <span class="keyword">if</span> sum(char_targets[offset1: offset2]) &gt; <span class="number">0</span>:</span><br><span class="line">            target_idx.append(j)</span><br><span class="line">    </span><br><span class="line">    targets_start = target_idx[<span class="number">0</span>]</span><br><span class="line">    targets_end = target_idx[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    sentiment_id = &#123;</span><br><span class="line">        <span class="string">'positive'</span>: <span class="number">1313</span>,</span><br><span class="line">        <span class="string">'negative'</span>: <span class="number">2430</span>,</span><br><span class="line">        <span class="string">'neutral'</span>: <span class="number">7974</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    input_ids = [<span class="number">0</span>] + [sentiment_id[sentiment]] + [<span class="number">2</span>] + [<span class="number">2</span>] + input_ids_orig + [<span class="number">2</span>]</span><br><span class="line">    token_type_ids = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>] + [<span class="number">0</span>] * (len(input_ids_orig) + <span class="number">1</span>)</span><br><span class="line">    mask = [<span class="number">1</span>] * len(token_type_ids)</span><br><span class="line">    tweet_offsets = [(<span class="number">0</span>, <span class="number">0</span>)] * <span class="number">4</span> + tweet_offsets + [(<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">    targets_start += <span class="number">4</span></span><br><span class="line">    targets_end += <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    padding_length = max_len - len(input_ids)</span><br><span class="line">    <span class="keyword">if</span> padding_length &gt; <span class="number">0</span>:</span><br><span class="line">        input_ids = input_ids + ([<span class="number">1</span>] * padding_length)</span><br><span class="line">        mask = mask + ([<span class="number">0</span>] * padding_length)</span><br><span class="line">        token_type_ids = token_type_ids + ([<span class="number">0</span>] * padding_length)</span><br><span class="line">        tweet_offsets = tweet_offsets + ([(<span class="number">0</span>, <span class="number">0</span>)] * padding_length)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'ids'</span>: input_ids,</span><br><span class="line">        <span class="string">'mask'</span>: mask,</span><br><span class="line">        <span class="string">'token_type_ids'</span>: token_type_ids,</span><br><span class="line">        <span class="string">'targets_start'</span>: targets_start,</span><br><span class="line">        <span class="string">'targets_end'</span>: targets_end,</span><br><span class="line">        <span class="string">'orig_tweet'</span>: tweet,</span><br><span class="line">        <span class="string">'orig_selected'</span>: selected_text,</span><br><span class="line">        <span class="string">'sentiment'</span>: sentiment,</span><br><span class="line">        <span class="string">'offsets'</span>: tweet_offsets</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Data-loader"><a href="#Data-loader" class="headerlink" title="Data loader"></a>Data loader</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TweetDataset</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Dataset which stores the tweets and returns them as processed features</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tweet, sentiment, selected_text)</span>:</span></span><br><span class="line">        self.tweet = tweet</span><br><span class="line">        self.sentiment = sentiment</span><br><span class="line">        self.selected_text = selected_text</span><br><span class="line">        self.tokenizer = config.TOKENIZER</span><br><span class="line">        self.max_len = config.MAX_LEN</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.tweet)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        data = process_data(</span><br><span class="line">            self.tweet[item], </span><br><span class="line">            self.selected_text[item], </span><br><span class="line">            self.sentiment[item],</span><br><span class="line">            self.tokenizer,</span><br><span class="line">            self.max_len</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Return the processed data where the lists are converted to `torch.tensor`s</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'ids'</span>: torch.tensor(data[<span class="string">"ids"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'mask'</span>: torch.tensor(data[<span class="string">"mask"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'token_type_ids'</span>: torch.tensor(data[<span class="string">"token_type_ids"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'targets_start'</span>: torch.tensor(data[<span class="string">"targets_start"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'targets_end'</span>: torch.tensor(data[<span class="string">"targets_end"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'orig_tweet'</span>: data[<span class="string">"orig_tweet"</span>],</span><br><span class="line">            <span class="string">'orig_selected'</span>: data[<span class="string">"orig_selected"</span>],</span><br><span class="line">            <span class="string">'sentiment'</span>: data[<span class="string">"sentiment"</span>],</span><br><span class="line">            <span class="string">'offsets'</span>: torch.tensor(data[<span class="string">"offsets"</span>], dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h1 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TweetModel</span><span class="params">(transformers.BertPreTrainedModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conf)</span>:</span></span><br><span class="line">        super(TweetModel, self).__init__(conf)</span><br><span class="line">        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)</span><br><span class="line">        self.drop_out = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.l_1 = nn.Linear(<span class="number">768</span> * <span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">        self.l0 = nn.Linear(<span class="number">400</span>, <span class="number">2</span>)</span><br><span class="line">        torch.nn.init.normal_(self.l0.weight, std=<span class="number">0.02</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ids, mask, token_type_ids)</span>:</span></span><br><span class="line">        _, _, out = self.roberta(</span><br><span class="line">            ids,</span><br><span class="line">            attention_mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        out = torch.cat((out[<span class="number">-1</span>], out[<span class="number">-2</span>]), dim=<span class="number">-1</span>)</span><br><span class="line">        out = self.drop_out(out)</span><br><span class="line">        logits = self.l_1(out)</span><br><span class="line">        logits = self.l0(logits)</span><br><span class="line"></span><br><span class="line">        start_logits, end_logits = logits.split(<span class="number">1</span>, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        start_logits = start_logits.squeeze(<span class="number">-1</span>)</span><br><span class="line">        end_logits = end_logits.squeeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> start_logits, end_logits</span><br></pre></td></tr></table></figure>
<h1 id="自定义损失函数（optional-取决于任务）"><a href="#自定义损失函数（optional-取决于任务）" class="headerlink" title="自定义损失函数（optional,取决于任务）"></a>自定义损失函数（optional,取决于任务）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(start_logits, end_logits, start_positions, end_positions)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return the sum of the cross entropy losses for both the start and end logits</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">    start_loss = loss_fct(start_logits, start_positions)</span><br><span class="line">    end_loss = loss_fct(end_logits, end_positions)</span><br><span class="line">    total_loss = (start_loss + end_loss)</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<h1 id="Training-Function"><a href="#Training-Function" class="headerlink" title="Training Function"></a>Training Function</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">(data_loader, model, optimizer, device, scheduler=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the bert model on the twitter data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Set model to training mode (dropout + sampled batchnorm is activated)</span></span><br><span class="line">    model.train()</span><br><span class="line">    losses = utils.AverageMeter()</span><br><span class="line">    jaccards = utils.AverageMeter()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set tqdm to add loading screen and set the length</span></span><br><span class="line">    tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Train the model on each batch</span></span><br><span class="line">    <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line"></span><br><span class="line">        ids = d[<span class="string">"ids"</span>]</span><br><span class="line">        token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">        mask = d[<span class="string">"mask"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">        orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">        orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        offsets = d[<span class="string">"offsets"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move ids, masks, and targets to gpu while setting as torch.long</span></span><br><span class="line">        ids = ids.to(device, dtype=torch.long)</span><br><span class="line">        token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">        mask = mask.to(device, dtype=torch.long)</span><br><span class="line">        targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">        targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reset gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        <span class="comment"># Use ids, masks, and token types as input to the model</span></span><br><span class="line">        <span class="comment"># Predict logits for each of the input tokens for each batch</span></span><br><span class="line">        outputs_start, outputs_end = model(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">        ) <span class="comment"># (bs x SL), (bs x SL)</span></span><br><span class="line">        <span class="comment"># Calculate batch loss based on CrossEntropy</span></span><br><span class="line">        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)</span><br><span class="line">        <span class="comment"># Calculate gradients based on loss</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># Adjust weights based on calculated gradients</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># Update scheduler</span></span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Apply softmax to the start and end logits</span></span><br><span class="line">        <span class="comment"># This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1</span></span><br><span class="line">        <span class="comment"># This is similar to the characteristics of "probabilities"</span></span><br><span class="line">        outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate the jaccard score based on the predictions for this batch</span></span><br><span class="line">        jaccard_scores = []</span><br><span class="line">        <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">            selected_tweet = orig_selected[px]</span><br><span class="line">            tweet_sentiment = sentiment[px]</span><br><span class="line">            jaccard_score, _ = calculate_jaccard_score(</span><br><span class="line">                original_tweet=tweet, <span class="comment"># Full text of the px'th tweet in the batch</span></span><br><span class="line">                target_string=selected_tweet, <span class="comment"># Span containing the specified sentiment for the px'th tweet in the batch</span></span><br><span class="line">                sentiment_val=tweet_sentiment, <span class="comment"># Sentiment of the px'th tweet in the batch</span></span><br><span class="line">                idx_start=np.argmax(outputs_start[px, :]), <span class="comment"># Predicted start index for the px'th tweet in the batch</span></span><br><span class="line">                idx_end=np.argmax(outputs_end[px, :]), <span class="comment"># Predicted end index for the px'th tweet in the batch</span></span><br><span class="line">                offsets=offsets[px] <span class="comment"># Offsets for each of the tokens for the px'th tweet in the batch</span></span><br><span class="line">            )</span><br><span class="line">            jaccard_scores.append(jaccard_score)</span><br><span class="line">        <span class="comment"># Update the jaccard score and loss</span></span><br><span class="line">        <span class="comment"># For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils</span></span><br><span class="line">        jaccards.update(np.mean(jaccard_scores), ids.size(<span class="number">0</span>))</span><br><span class="line">        losses.update(loss.item(), ids.size(<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># Print the average loss and jaccard score at the end of each batch</span></span><br><span class="line">        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)</span><br></pre></td></tr></table></figure>
<h1 id="Evaluation-Functions"><a href="#Evaluation-Functions" class="headerlink" title="Evaluation Functions"></a>Evaluation Functions</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_jaccard_score</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    original_tweet, </span></span></span><br><span class="line"><span class="function"><span class="params">    target_string, </span></span></span><br><span class="line"><span class="function"><span class="params">    sentiment_val, </span></span></span><br><span class="line"><span class="function"><span class="params">    idx_start, </span></span></span><br><span class="line"><span class="function"><span class="params">    idx_end, </span></span></span><br><span class="line"><span class="function"><span class="params">    offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">    verbose=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># A span's start index has to be greater than or equal to the end index</span></span><br><span class="line">    <span class="comment"># If this doesn't hold, the start index is set to equal the end index (the span is a single token)</span></span><br><span class="line">    <span class="keyword">if</span> idx_end &lt; idx_start:</span><br><span class="line">        idx_end = idx_start</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Combine into a string the tokens that belong to the predicted span</span></span><br><span class="line">    filtered_output  = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> ix <span class="keyword">in</span> range(idx_start, idx_end + <span class="number">1</span>):</span><br><span class="line">        filtered_output += original_tweet[offsets[ix][<span class="number">0</span>]: offsets[ix][<span class="number">1</span>]]</span><br><span class="line">        <span class="comment"># If the token is not the last token in the tweet, and the ending offset of the current token is less</span></span><br><span class="line">        <span class="comment"># than the beginning offset of the following token, add a space.</span></span><br><span class="line">        <span class="comment"># Basically, add a space when the next token (word piece) corresponds to a new word</span></span><br><span class="line">        <span class="keyword">if</span> (ix+<span class="number">1</span>) &lt; len(offsets) <span class="keyword">and</span> offsets[ix][<span class="number">1</span>] &lt; offsets[ix+<span class="number">1</span>][<span class="number">0</span>]:</span><br><span class="line">            filtered_output += <span class="string">" "</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the predicted output as the original tweet when the tweet's sentiment is "neutral", or the tweet only contains one word</span></span><br><span class="line">    <span class="keyword">if</span> sentiment_val == <span class="string">"neutral"</span> <span class="keyword">or</span> len(original_tweet.split()) &lt; <span class="number">2</span>:</span><br><span class="line">        filtered_output = original_tweet</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the jaccard score between the predicted span, and the actual span</span></span><br><span class="line">    <span class="comment"># The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:</span></span><br><span class="line">    <span class="comment"># https://www.kaggle.com/abhishek/utils</span></span><br><span class="line">    jac = utils.jaccard(target_string.strip(), filtered_output.strip())</span><br><span class="line">    <span class="keyword">return</span> jac, filtered_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_fn</span><span class="params">(data_loader, model, device)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Evaluation function to predict on the test set</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Set model to evaluation mode</span></span><br><span class="line">    <span class="comment"># I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance</span></span><br><span class="line">    <span class="comment"># Reference: https://github.com/pytorch/pytorch/issues/5406</span></span><br><span class="line">    model.eval()</span><br><span class="line">    losses = utils.AverageMeter()</span><br><span class="line">    jaccards = utils.AverageMeter()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">        <span class="comment"># Make predictions and calculate loss / jaccard score for each batch</span></span><br><span class="line">        <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line">            ids = d[<span class="string">"ids"</span>]</span><br><span class="line">            token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">            mask = d[<span class="string">"mask"</span>]</span><br><span class="line">            sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">            orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">            orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">            targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">            targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">            offsets = d[<span class="string">"offsets"</span>].numpy()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Move tensors to GPU for faster matrix calculations</span></span><br><span class="line">            ids = ids.to(device, dtype=torch.long)</span><br><span class="line">            token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">            mask = mask.to(device, dtype=torch.long)</span><br><span class="line">            targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">            targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Predict logits for start and end indexes</span></span><br><span class="line">            outputs_start, outputs_end = model(</span><br><span class="line">                ids=ids,</span><br><span class="line">                mask=mask,</span><br><span class="line">                token_type_ids=token_type_ids</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># Calculate loss for the batch</span></span><br><span class="line">            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)</span><br><span class="line">            <span class="comment"># Apply softmax to the predicted logits for the start and end indexes</span></span><br><span class="line">            <span class="comment"># This converts the "logits" to "probability-like" scores</span></span><br><span class="line">            outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">            outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">            <span class="comment"># Calculate jaccard scores for each tweet in the batch</span></span><br><span class="line">            jaccard_scores = []</span><br><span class="line">            <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">                selected_tweet = orig_selected[px]</span><br><span class="line">                tweet_sentiment = sentiment[px]</span><br><span class="line">                jaccard_score, _ = calculate_jaccard_score(</span><br><span class="line">                    original_tweet=tweet,</span><br><span class="line">                    target_string=selected_tweet,</span><br><span class="line">                    sentiment_val=tweet_sentiment,</span><br><span class="line">                    idx_start=np.argmax(outputs_start[px, :]),</span><br><span class="line">                    idx_end=np.argmax(outputs_end[px, :]),</span><br><span class="line">                    offsets=offsets[px]</span><br><span class="line">                )</span><br><span class="line">                jaccard_scores.append(jaccard_score)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update running jaccard score and loss</span></span><br><span class="line">            jaccards.update(np.mean(jaccard_scores), ids.size(<span class="number">0</span>))</span><br><span class="line">            losses.update(loss.item(), ids.size(<span class="number">0</span>))</span><br><span class="line">            <span class="comment"># Print the running average loss and jaccard score</span></span><br><span class="line">            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f"Jaccard = <span class="subst">&#123;jaccards.avg&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">return</span> jaccards.avg</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(fold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train model for a speciied fold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Read training csv</span></span><br><span class="line">    dfx = pd.read_csv(config.TRAINING_FILE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set train validation set split</span></span><br><span class="line">    df_train = dfx[dfx.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = dfx[dfx.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Instantiate TweetDataset with training data</span></span><br><span class="line">    train_dataset = TweetDataset(</span><br><span class="line">        tweet=df_train.text.values,</span><br><span class="line">        sentiment=df_train.sentiment.values,</span><br><span class="line">        selected_text=df_train.selected_text.values</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate DataLoader with `train_dataset`</span></span><br><span class="line">    <span class="comment"># This is a generator that yields the dataset in batches</span></span><br><span class="line">    train_data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        train_dataset,</span><br><span class="line">        batch_size=config.TRAIN_BATCH_SIZE,</span><br><span class="line">        num_workers=<span class="number">4</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate TweetDataset with validation data</span></span><br><span class="line">    valid_dataset = TweetDataset(</span><br><span class="line">        tweet=df_valid.text.values,</span><br><span class="line">        sentiment=df_valid.sentiment.values,</span><br><span class="line">        selected_text=df_valid.selected_text.values</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate DataLoader with `valid_dataset`</span></span><br><span class="line">    valid_data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        valid_dataset,</span><br><span class="line">        batch_size=config.VALID_BATCH_SIZE,</span><br><span class="line">        num_workers=<span class="number">2</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set device as `cuda` (GPU)</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    <span class="comment"># Load pretrained BERT (bert-base-uncased)</span></span><br><span class="line">    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)</span><br><span class="line">    <span class="comment"># Output hidden states</span></span><br><span class="line">    <span class="comment"># This is important to set since we want to concatenate the hidden states from the last 2 BERT layers</span></span><br><span class="line">    model_config.output_hidden_states = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># Instantiate our model with `model_config`</span></span><br><span class="line">    model = TweetModel(conf=model_config)</span><br><span class="line">    <span class="comment"># Move the model to the GPU</span></span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the number of training steps</span></span><br><span class="line">    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)</span><br><span class="line">    <span class="comment"># Get the list of named parameters</span></span><br><span class="line">    param_optimizer = list(model.named_parameters())</span><br><span class="line">    <span class="comment"># Specify parameters where weight decay shouldn't be applied</span></span><br><span class="line">    no_decay = [<span class="string">"bias"</span>, <span class="string">"LayerNorm.bias"</span>, <span class="string">"LayerNorm.weight"</span>]</span><br><span class="line">    <span class="comment"># Define two sets of parameters: those with weight decay, and those without</span></span><br><span class="line">    optimizer_parameters = [</span><br><span class="line">        &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="keyword">not</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5</span></span><br><span class="line">    optimizer = AdamW(optimizer_parameters, lr=<span class="number">3e-5</span>)</span><br><span class="line">    <span class="comment"># Create a scheduler to set the learning rate at each training step</span></span><br><span class="line">    <span class="comment"># "Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period." (https://pytorch.org/docs/stable/optim.html)</span></span><br><span class="line">    <span class="comment"># Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step</span></span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer, </span><br><span class="line">        num_warmup_steps=<span class="number">0</span>, </span><br><span class="line">        num_training_steps=num_train_steps</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply early stopping with patience of 2</span></span><br><span class="line">    <span class="comment"># This means to stop training new epochs when 2 rounds have passed without any improvement</span></span><br><span class="line">    es = utils.EarlyStopping(patience=<span class="number">2</span>, mode=<span class="string">"max"</span>)</span><br><span class="line">    print(<span class="string">f"Training is Starting for fold=<span class="subst">&#123;fold&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># I'm training only for 3 epochs even though I specified 5!!!</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)</span><br><span class="line">        jaccard = eval_fn(valid_data_loader, model, device)</span><br><span class="line">        print(<span class="string">f"Jaccard Score = <span class="subst">&#123;jaccard&#125;</span>"</span>)</span><br><span class="line">        es(jaccard, model, model_path=<span class="string">f"model_<span class="subst">&#123;fold&#125;</span>.bin"</span>)</span><br><span class="line">        <span class="keyword">if</span> es.early_stop:</span><br><span class="line">            print(<span class="string">"Early stopping"</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Do-the-evaluation-on-test-data"><a href="#Do-the-evaluation-on-test-data" class="headerlink" title="Do the evaluation on test data"></a>Do the evaluation on test data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_test = pd.read_csv(<span class="string">"../input/tweet-sentiment-extraction/test.csv"</span>)</span><br><span class="line">df_test.loc[:, <span class="string">"selected_text"</span>] = df_test.text.values</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_test</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>textID</th>
      <th>text</th>
      <th>sentiment</th>
      <th>selected_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>f87dea47db</td>
      <td>Last session of the day  http://twitpic.com/67ezh</td>
      <td>neutral</td>
      <td>Last session of the day  http://twitpic.com/67ezh</td>
    </tr>
    <tr>
      <th>1</th>
      <td>96d74cb729</td>
      <td>Shanghai is also really exciting (precisely -...</td>
      <td>positive</td>
      <td>Shanghai is also really exciting (precisely -...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>eee518ae67</td>
      <td>Recession hit Veronique Branquinho, she has to...</td>
      <td>negative</td>
      <td>Recession hit Veronique Branquinho, she has to...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01082688c6</td>
      <td>happy bday!</td>
      <td>positive</td>
      <td>happy bday!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33987a8ee5</td>
      <td>http://twitpic.com/4w75p - I like it!!</td>
      <td>positive</td>
      <td>http://twitpic.com/4w75p - I like it!!</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3529</th>
      <td>e5f0e6ef4b</td>
      <td>its at 3 am, im very tired but i can`t sleep  ...</td>
      <td>negative</td>
      <td>its at 3 am, im very tired but i can`t sleep  ...</td>
    </tr>
    <tr>
      <th>3530</th>
      <td>416863ce47</td>
      <td>All alone in this old house again.  Thanks for...</td>
      <td>positive</td>
      <td>All alone in this old house again.  Thanks for...</td>
    </tr>
    <tr>
      <th>3531</th>
      <td>6332da480c</td>
      <td>I know what you mean. My little dog is sinkin...</td>
      <td>negative</td>
      <td>I know what you mean. My little dog is sinkin...</td>
    </tr>
    <tr>
      <th>3532</th>
      <td>df1baec676</td>
      <td>_sutra what is your next youtube video gonna b...</td>
      <td>positive</td>
      <td>_sutra what is your next youtube video gonna b...</td>
    </tr>
    <tr>
      <th>3533</th>
      <td>469e15c5a8</td>
      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>
      <td>positive</td>
      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>
    </tr>
  </tbody>
</table>
<p>3534 rows × 4 columns</p>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)</span><br><span class="line">model_config.output_hidden_states = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load each of the five trained models and move to GPU</span></span><br><span class="line">model1 = TweetModel(conf=model_config)</span><br><span class="line">model1.to(device)</span><br><span class="line">model1.load_state_dict(torch.load(<span class="string">"model_0.bin"</span>))</span><br><span class="line">model1.eval()</span><br><span class="line"></span><br><span class="line">model2 = TweetModel(conf=model_config)</span><br><span class="line">model2.to(device)</span><br><span class="line">model2.load_state_dict(torch.load(<span class="string">"model_1.bin"</span>))</span><br><span class="line">model2.eval()</span><br><span class="line"></span><br><span class="line">model3 = TweetModel(conf=model_config)</span><br><span class="line">model3.to(device)</span><br><span class="line">model3.load_state_dict(torch.load(<span class="string">"model_2.bin"</span>))</span><br><span class="line">model3.eval()</span><br><span class="line"></span><br><span class="line">model4 = TweetModel(conf=model_config)</span><br><span class="line">model4.to(device)</span><br><span class="line">model4.load_state_dict(torch.load(<span class="string">"model_3.bin"</span>))</span><br><span class="line">model4.eval()</span><br><span class="line"></span><br><span class="line">model5 = TweetModel(conf=model_config)</span><br><span class="line">model5.to(device)</span><br><span class="line">model5.load_state_dict(torch.load(<span class="string">"model_4.bin"</span>))</span><br><span class="line">model5.eval()</span><br></pre></td></tr></table></figure>
<pre><code>TweetModel(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (drop_out): Dropout(p=0.1, inplace=False)
  (l_1): Linear(in_features=1536, out_features=400, bias=True)
  (l0): Linear(in_features=400, out_features=2, bias=True)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">final_output = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate TweetDataset with the test data</span></span><br><span class="line">test_dataset = TweetDataset(</span><br><span class="line">        tweet=df_test.text.values,</span><br><span class="line">        sentiment=df_test.sentiment.values,</span><br><span class="line">        selected_text=df_test.selected_text.values</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate DataLoader with `test_dataset`</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    test_dataset,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    batch_size=config.VALID_BATCH_SIZE,</span><br><span class="line">    num_workers=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn of gradient calculations</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">    <span class="comment"># Predict the span containing the sentiment for each batch</span></span><br><span class="line">    <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line">        ids = d[<span class="string">"ids"</span>]</span><br><span class="line">        token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">        mask = d[<span class="string">"mask"</span>]</span><br><span class="line">        sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">        orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">        orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        offsets = d[<span class="string">"offsets"</span>].numpy()</span><br><span class="line"></span><br><span class="line">        ids = ids.to(device, dtype=torch.long)</span><br><span class="line">        token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">        mask = mask.to(device, dtype=torch.long)</span><br><span class="line">        targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">        targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Predict start and end logits for each of the five models</span></span><br><span class="line">        outputs_start1, outputs_end1 = model1(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start2, outputs_end2 = model2(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start3, outputs_end3 = model3(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start4, outputs_end4 = model4(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start5, outputs_end5 = model5(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the average start and end logits across the five models and use these as predictions</span></span><br><span class="line">        <span class="comment"># This is a form of "ensembling"</span></span><br><span class="line">        outputs_start = (</span><br><span class="line">            outputs_start1 </span><br><span class="line">            + outputs_start2 </span><br><span class="line">            + outputs_start3 </span><br><span class="line">            + outputs_start4 </span><br><span class="line">            + outputs_start5</span><br><span class="line">        ) / <span class="number">5</span></span><br><span class="line">        outputs_end = (</span><br><span class="line">            outputs_end1 </span><br><span class="line">            + outputs_end2 </span><br><span class="line">            + outputs_end3 </span><br><span class="line">            + outputs_end4 </span><br><span class="line">            + outputs_end5</span><br><span class="line">        ) / <span class="number">5</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Apply softmax to the predicted start and end logits</span></span><br><span class="line">        outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert the start and end scores to actual predicted spans (in string form)</span></span><br><span class="line">        <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">            selected_tweet = orig_selected[px]</span><br><span class="line">            tweet_sentiment = sentiment[px]</span><br><span class="line">            _, output_sentence = calculate_jaccard_score(</span><br><span class="line">                original_tweet=tweet,</span><br><span class="line">                target_string=selected_tweet,</span><br><span class="line">                sentiment_val=tweet_sentiment,</span><br><span class="line">                idx_start=np.argmax(outputs_start[px, :]),</span><br><span class="line">                idx_end=np.argmax(outputs_end[px, :]),</span><br><span class="line">                offsets=offsets[px]</span><br><span class="line">            )</span><br><span class="line">            final_output.append(output_sentence)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=221.0), HTML(value=&#39;&#39;)))
</code></pre><p>​<br>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># post-process trick:</span></span><br><span class="line"><span class="comment"># Note: This trick comes from: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942</span></span><br><span class="line"><span class="comment"># When the LB resets, this trick won't help</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_process</span><span class="params">(selected)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">" "</span>.join(set(selected.lower().split()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample = pd.read_csv(<span class="string">"../input/tweet-sentiment-extraction/sample_submission.csv"</span>)</span><br><span class="line">sample.loc[:, <span class="string">'selected_text'</span>] = final_output</span><br><span class="line">sample.selected_text = sample.selected_text.map(post_process)</span><br><span class="line">sample.to_csv(<span class="string">"submission.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>textID</th>
      <th>selected_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>f87dea47db</td>
      <td>http://twitpic.com/67ezh the of day session last</td>
    </tr>
    <tr>
      <th>1</th>
      <td>96d74cb729</td>
      <td>exciting</td>
    </tr>
    <tr>
      <th>2</th>
      <td>eee518ae67</td>
      <td>such shame! a</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01082688c6</td>
      <td>happy bday!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33987a8ee5</td>
      <td>i like</td>
    </tr>
  </tbody>
</table>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line">source_DF = inspect.getsource(utils)</span><br><span class="line">print(source_DF)</span><br></pre></td></tr></table></figure>
<pre><code>import numpy as np
import torch
</code></pre><p>​    </p>
<pre><code>class AverageMeter:
    &quot;&quot;&quot;
    Computes and stores the average and current value
    &quot;&quot;&quot;
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
</code></pre><p>​    </p>
<pre><code>class EarlyStopping:
    def __init__(self, patience=7, mode=&quot;max&quot;, delta=0.001):
        self.patience = patience
        self.counter = 0
        self.mode = mode
        self.best_score = None
        self.early_stop = False
        self.delta = delta
        if self.mode == &quot;min&quot;:
            self.val_score = np.Inf
        else:
            self.val_score = -np.Inf

    def __call__(self, epoch_score, model, model_path):

        if self.mode == &quot;min&quot;:
            score = -1.0 * epoch_score
        else:
            score = np.copy(epoch_score)

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
        elif score &lt; self.best_score + self.delta:
            self.counter += 1
            print(&#39;EarlyStopping counter: {} out of {}&#39;.format(self.counter, self.patience))
            if self.counter &gt;= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
            self.counter = 0

    def save_checkpoint(self, epoch_score, model, model_path):
        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:
            print(&#39;Validation score improved ({} --&gt; {}). Saving model!&#39;.format(self.val_score, epoch_score))
            torch.save(model.state_dict(), model_path)
        self.val_score = epoch_score
</code></pre><p>​    </p>
<pre><code>def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
</code></pre><p>​    </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/22/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B7%A5%E5%85%B7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="Learning in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄写字的地方">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/22/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B7%A5%E5%85%B7/" class="post-title-link" itemprop="url">算法设计基础-最重要的两个工具</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-22 15:02:12" itemprop="dateCreated datePublished" datetime="2020-04-22T15:02:12+08:00">2020-04-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 09:00:46" itemprop="dateModified" datetime="2020-04-23T09:00:46+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithms/" itemprop="url" rel="index"><span itemprop="name">algorithms</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="两个基础"><a href="#两个基础" class="headerlink" title="两个基础"></a>两个基础</h3><ol>
<li><p>算法设计中的一项重要的技术是缩小允许的实例的集合，直到找到正确有效的算法为止。</p>
</li>
<li><p>要利用好已有的算法，您必须学会基本过程中的“抽象地”描述问题。根据定义好的结构和算法对应用程序进行建模是实现解决方案的最重要的一步。</p>
</li>
</ol>
<h3 id="RAM-和-大O"><a href="#RAM-和-大O" class="headerlink" title="RAM 和 大O"></a>RAM 和 大O</h3><ol>
<li><p>RAM计算模型</p>
<p>将机器抽象为一个简单的机器，在这个机器上进行计算：</p>
<ul>
<li>每个简单的操作（+，*，-，=，if，call）只需一个时间步。</li>
<li>循环和子例程不被视为简单操作</li>
<li>每次内存访问仅需一个时间步长，而无需注意缓存或磁盘中的内容</li>
</ul>
</li>
<li><p>大O</p>
<p>这些复杂性中的每一个都定义了一个数字函数，表示时间与问题大小的关系。但是时间上的复杂性是如此复杂，以至于我们必须简化它们以实现利用。为此，我们需要使用大O符号</p>
<p>首先，要精确计算复杂度非常困难，因为它倾向于：</p>
</li>
</ol>
<ul>
<li><p>变化很大，不稳定</p>
</li>
<li><p>需要太多细节才能精确指定。因为计算最坏情况下执行的RAM指令的确切数量需要把整个计算机程序的细节考虑进来。</p>
<p> 大O符号会精确到不影响我们算法比较的详细程度。</p>
<h3 id="一个原则"><a href="#一个原则" class="headerlink" title="一个原则"></a>一个原则</h3></li>
<li><p>最重要的原则：<strong>Big Oh符号</strong>和<strong>最坏情况分析</strong>是极大简化我们比较算法效率的功能的工具。</p>
</li>
<li><p>大O的通俗解法：找一个c，不管n怎么变大，$g(n)$都是其上界。注意，$g(n)$是比较大的那一个</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">董雄</p>
  <div class="site-description" itemprop="description">Learning in Harbin Institute of Technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">董雄</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1278837449&web_id=1278837449"></script>
  </div>






      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 't8n8RTilca5Gm0vKToaMrjRU-gzGzoHsz',
      appKey     : 'x4Jty8MrDpczjPANtbbGhwXX',
      placeholder: "请留下一点痕迹吧, 评论将永远留存",
      avatar     : 'robohash',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
