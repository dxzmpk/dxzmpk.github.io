<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/%E8%A7%86%E5%8A%9B%E8%A1%A8%20(1).svg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/%E8%A7%86%E5%8A%9B%E8%A1%A8.svg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dxzmpk.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
<meta property="og:type" content="website">
<meta property="og:title" content="董雄的博客">
<meta property="og:url" content="https://dxzmpk.github.io/">
<meta property="og:site_name" content="董雄的博客">
<meta property="og:description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="董雄">
<meta property="article:tag" content="nlp">
<meta property="article:tag" content=" cs">
<meta property="article:tag" content=" hit">
<meta property="article:tag" content=" transformers">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://dxzmpk.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>董雄的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">董雄的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">7分nlp,3分计算机基础</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/23/HuggingFace-transformers%E7%B3%BB%E5%88%97%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%9C%A8%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/23/HuggingFace-transformers%E7%B3%BB%E5%88%97%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%9C%A8%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">HuggingFace-Transformers系列的介绍以及在下游任务中的使用</a>
        </h2>

        <div class="post-meta">
          
            <i class="fa fa-thumb-tack"></i>
            <font color=7D26CD>置顶</font>
            <span class="post-meta-divider">|</span>
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-23 09:02:46" itemprop="dateCreated datePublished" datetime="2020-04-23T09:02:46+08:00">2020-04-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-05 17:30:03" itemprop="dateModified" datetime="2020-05-05T17:30:03+08:00">2020-05-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="内容介绍"><a href="#内容介绍" class="headerlink" title="内容介绍"></a>内容介绍</h1><p>这篇博客主要面向对<strong>Bert</strong>系列在<strong>Pytorch</strong>上应用感兴趣的同学，将涵盖的主要内容是：Bert系列有关的论文，<a href="https://huggingface.co/transformers/installation.html#" target="_blank" rel="noopener">Huggingface</a>的实现，以及如何在不同下游任务中使用预训练模型。</p>
<p>看过这篇博客，你将了解：</p>
<ul>
<li>Transformers实现的介绍，不同的Tokenizer和Model如何使用。</li>
<li>如何利用HuggingFace的实现自定义你的模型，如果你想利用这个库实现自己的下游任务，而不想过多关注其实现细节的话，那么这篇文章将会成为很好的参考。</li>
</ul>
<h2 id="Huggingface-transformers介绍"><a href="#Huggingface-transformers介绍" class="headerlink" title="Huggingface-transformers介绍"></a>Huggingface-transformers介绍</h2><p><a href="https://huggingface.co/transformers/index.html" target="_blank" rel="noopener">transformers</a>（以前称为pytorch-transformers和pytorch-pretrained-bert）提供用于自然语言理解（NLU）和自然语言生成（NLG）的BERT家族通用结构（BERT，GPT-2，RoBERTa，XLM，DistilBert，XLNet等），包含超过32种、涵盖100多种语言的预训练模型。同时提供TensorFlow 2.0和PyTorch之间的高互通性。</p>
<p><strong>特性</strong>：</p>
<ul>
<li><p>与pytorch-transformers一样易于使用</p>
</li>
<li><p>像Keras一样强大而简洁</p>
</li>
<li><p>在NLU和NLG任务上表现良好</p>
</li>
<li><p>对于教育者和从业者的门槛低</p>
</li>
</ul>
<p><strong>现存的模型</strong>：</p>
<ul>
<li><p>Bert(基础版和巨人版, 是否区分大小写)， </p>
</li>
<li><p>GPT, GPT-2</p>
</li>
<li><p>Transformer-XL, XLNet, XLM</p>
</li>
<li><p>DistilBERT, DistilGPT2</p>
</li>
<li><p>CTRL</p>
</li>
<li><p>ALBERT, RoBERTa, XLM-RoBERTa</p>
</li>
<li><p>FlauBERT，CamemBERT</p>
</li>
<li>其他在各种下游任务上微调过的模型。</li>
<li>在多语言上训练的模型</li>
</ul>
<h1 id="所需的知识"><a href="#所需的知识" class="headerlink" title="所需的知识"></a>所需的知识</h1><p><a href="https://huggingface.co/transformers/installation.html#" target="_blank" rel="noopener">安装Huggface库</a>(需要预先安装pytorch)</p>
<p>在阅读这篇文章之前，如果你能将以下资料读一遍，或者看一遍的话，在后续的阅读过程中将极大地减少你陷入疑惑的概率。</p>
<ul>
<li>视频类内容：根据排序观看更佳<ul>
<li><a href="https://www.bilibili.com/video/BV17441137fa?from=search&amp;seid=17871853660365597030" target="_blank" rel="noopener">李宏毅关于Elmo, Bert, GPT的讲解</a></li>
<li><a href="https://www.bilibili.com/video/BV1Tt411M7Wp?from=search&amp;seid=194654766718455422" target="_blank" rel="noopener">Goebels关于transformerXL的讲解</a></li>
<li><a href="https://www.youtube.com/watch?v=H5vpBCLo74U" target="_blank" rel="noopener">Kilcher关于XLnet的讲解</a></li>
<li><a href="https://www.youtube.com/watch?v=vsGN8WqwvKg&amp;t=597s" target="_blank" rel="noopener">McCormick关于ALBERT的讲解</a></li>
</ul>
</li>
</ul>
<p><strong>或者</strong>，你更愿意去看论文的话：</p>
<ul>
<li>相关论文：根据排序阅读更佳<ul>
<li><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT论文</a>, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</li>
<li><a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener">Transformer-XL论文</a>, Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,  Authors: Zihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V. Le and Ruslan Salakhutdinov.</li>
<li><a href="http://xxx.itp.ac.cn/abs/1906.08237" target="_blank" rel="noopener">XLNet论文</a>，XLNet: Generalized Autoregressive Pretraining for Language Understanding</li>
<li><a href="http://xxx.itp.ac.cn/abs/1909.11942" target="_blank" rel="noopener">ALBERT论文</a>，ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</li>
<li><a href="http://xxx.itp.ac.cn/abs/1907.11692" target="_blank" rel="noopener">RoBERTa论文</a>， RoBERTa: A Robustly Optimized BERT Pretraining Approach</li>
<li><a href="http://xxx.itp.ac.cn/abs/1910.01108" target="_blank" rel="noopener">DistilBERT论文</a>，DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</li>
</ul>
</li>
</ul>
<h1 id="HuggingFace模型加载-下游任务使用"><a href="#HuggingFace模型加载-下游任务使用" class="headerlink" title="HuggingFace模型加载+下游任务使用"></a>HuggingFace模型加载+下游任务使用</h1><h2 id="项目组件"><a href="#项目组件" class="headerlink" title="项目组件"></a>项目组件</h2><p>一个完整的transformer模型主要包含三部分：</p>
<ol>
<li><p><strong>Config</strong>，控制模型的名称、最终输出的样式、隐藏层宽度和深度、激活函数的类别等。将Config类导出时文件格式为 json格式，就像下面这样：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"attention_probs_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_act"</span>: <span class="string">"gelu"</span>,</span><br><span class="line">  <span class="attr">"hidden_dropout_prob"</span>: <span class="number">0.1</span>,</span><br><span class="line">  <span class="attr">"hidden_size"</span>: <span class="number">768</span>,</span><br><span class="line">  <span class="attr">"initializer_range"</span>: <span class="number">0.02</span>,</span><br><span class="line">  <span class="attr">"intermediate_size"</span>: <span class="number">3072</span>,</span><br><span class="line">  <span class="attr">"max_position_embeddings"</span>: <span class="number">512</span>,</span><br><span class="line">  <span class="attr">"num_attention_heads"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"num_hidden_layers"</span>: <span class="number">12</span>,</span><br><span class="line">  <span class="attr">"type_vocab_size"</span>: <span class="number">2</span>,</span><br><span class="line">  <span class="attr">"vocab_size"</span>: <span class="number">30522</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然，也可以通过config.json来实例化Config类，这是一个互逆的过程。</p>
</li>
<li><p><strong>Tokenizer</strong>，这是一个将纯文本转换为编码的过程。注意，Tokenizer并不涉及将词转化为词向量的过程，仅仅是将纯文本分词，添加[MASK]标记、[SEP]、[CLS]标记，并转换为字典索引。Tokenizer类导出时将分为三个文件，也就是：</p>
<ul>
<li><p>vocab.txt</p>
<p>词典文件，每一行为一个词或词的一部分</p>
</li>
<li><p>special_tokens_map.json 特殊标记的定义方式</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"unk_token"</span>: <span class="string">"[UNK]"</span>, <span class="attr">"sep_token"</span>: <span class="string">"[SEP]"</span>, <span class="attr">"pad_token"</span>: <span class="string">"[PAD]"</span>, </span><br><span class="line"> <span class="attr">"cls_token"</span>: <span class="string">"[CLS]"</span>, <span class="attr">"mask_token"</span>: <span class="string">"[MASK]"</span>&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>tokenizer_config.json 配置文件，主要存储特殊的配置。</p>
</li>
</ul>
</li>
<li><p><strong>Model</strong>，也就是各种各样的模型。除了初始的Bert、GPT等基本模型，针对下游任务，还定义了诸如<code>BertForQuestionAnswering</code>等下游任务模型。模型导出时将生成<code>config.json</code>和<code>pytorch_model.bin</code>参数文件。前者就是1中的配置文件，这和我们的直觉相同，即config和model应该是紧密联系在一起的两个类。后者其实和torch.save()存储得到的文件是相同的，这是因为Model都直接或者间接继承了Pytorch的Module类。从这里可以看出，HuggingFace在实现时很好地尊重了Pytorch的原生API。</p>
</li>
</ol>
<h2 id="导入Bert系列基本模型的方法"><a href="#导入Bert系列基本模型的方法" class="headerlink" title="导入Bert系列基本模型的方法"></a>导入Bert系列基本模型的方法</h2><h3 id="通过官网自动导入"><a href="#通过官网自动导入" class="headerlink" title="通过官网自动导入"></a>通过官网自动导入</h3><p>官方文档中<a href="https://huggingface.co/transformers/quickstart.html#" target="_blank" rel="noopener">初始教程</a>提供的方法为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from transformers import BertModel</span></span><br><span class="line"><span class="comment"># Load pre-trained model (weights)</span></span><br><span class="line"><span class="comment"># model = BertModel.from_pretrained('bert-base-uncased')</span></span><br></pre></td></tr></table></figure>
<p>这个方法需要从官方的s3数据库下载模型配置、参数等信息（代码中已配置好位置）。这个方法虽然简单，但是在国内并不可用。当然你可以先尝试一下，不过会有很大的概率无法下载模型。</p>
<h3 id="手动下载模型信息并导入"><a href="#手动下载模型信息并导入" class="headerlink" title="手动下载模型信息并导入"></a>手动下载模型信息并导入</h3><ol>
<li><p>在HuggingFace<a href="https://huggingface.co/models" target="_blank" rel="noopener">官方模型库</a>上找到需要下载的模型，点击模型链接， 这个例子使用的是bert-base-uncased模型</p>
</li>
<li><p>点击<a href="https://huggingface.co/bert-base-uncased#" target="_blank" rel="noopener">List all files in model</a>，将其中的文件一一下载到同一目录中。例如，对于XLNet:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># List of model files</span><br><span class="line">config.json	782.0B</span><br><span class="line">pytorch_model.bin	445.4MB</span><br><span class="line">special_tokens_map.json	202.0B</span><br><span class="line">spiece.model	779.3KB</span><br><span class="line">tokenizer_config.json	2.0B</span><br></pre></td></tr></table></figure>
<p>但是这种方法有时也会不可用。如果您可以将Transformers预训练模型上传到迅雷等网盘的话，请在评论区告知，我会添加在此博客中，并为您添加博客友链。</p>
</li>
<li><p>通过下载好的路径导入模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line">MODEL_PATH = <span class="string">r"D:\transformr_files\bert-base-uncased/"</span></span><br><span class="line"><span class="comment"># a.通过词典导入分词器</span></span><br><span class="line">tokenizer = transformers.BertTokenizer.from_pretrained(<span class="string">r"D:\transformr_files\bert-base-uncased\bert-base-uncased-vocab.txt"</span>) </span><br><span class="line"><span class="comment"># b. 导入配置文件</span></span><br><span class="line">model_config = transformers.BertConfig.from_pretrained(MODEL_PATH)</span><br><span class="line"><span class="comment"># 修改配置</span></span><br><span class="line">model_config.output_hidden_states = <span class="literal">True</span></span><br><span class="line">model_config.output_attentions = <span class="literal">True</span></span><br><span class="line"><span class="comment"># 通过配置和路径导入模型</span></span><br><span class="line">model = transformers.BertModel.from_pretrained(MODEL_PATH,config = model_config)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="利用分词器分词"><a href="#利用分词器分词" class="headerlink" title="利用分词器分词"></a>利用分词器分词</h3><p>利用分词器进行编码</p>
<ul>
<li><p>对于单句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode仅返回input_ids</span></span><br><span class="line">tokenizer.encode(<span class="string">"i like you"</span>)</span><br><span class="line">Out : [<span class="number">101</span>, <span class="number">1045</span>, <span class="number">2066</span>, <span class="number">2017</span>, <span class="number">102</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>对于多句：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># encode_plus返回所有编码信息</span></span><br><span class="line">sen_code = tokenizer.encode_plus(<span class="string">"i like you"</span>, <span class="string">"but not him"</span>)</span><br><span class="line">Out : </span><br><span class="line">    &#123;<span class="string">'input_ids'</span>: [<span class="number">101</span>, <span class="number">1045</span>, <span class="number">2066</span>, <span class="number">2017</span>, <span class="number">102</span>, <span class="number">2021</span>, <span class="number">2025</span>, <span class="number">2032</span>, <span class="number">102</span>],</span><br><span class="line">     <span class="string">'token_type_ids'</span>: [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">     <span class="string">'attention_mask'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]&#125;</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>模型的所有分词器都是在PreTrainedTokenizer中实现的，分词的结果主要有以下内容：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">input_ids: list[int],</span><br><span class="line">token_type_ids: list[int] if return_token_type_ids is True (default)</span><br><span class="line">attention_mask: list[int] if return_attention_mask is True (default)</span><br><span class="line">overflowing_tokens: list[int] if a max_length is specified and 		return_overflowing_tokens is True</span><br><span class="line">num_truncated_tokens: int if a max_length is specified and return_overflowing_tokens is True</span><br><span class="line">special_tokens_mask: list[int] if add_special_tokens if set to True and return_special_tokens_mask is True</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>编码解释：</p>
<ul>
<li>‘input_ids’：顾名思义，是单词在词典中的编码</li>
<li>‘token_type_ids’， 区分两个句子的编码</li>
<li>‘attention_mask’,  指定对哪些词进行self-Attention操作</li>
<li>‘overflowing_tokens’, 当指定最大长度时，溢出的单词</li>
<li>‘num_truncated_tokens’, 溢出的token数量</li>
<li>‘return_special_tokens_mask’，如果添加特殊标记，则这是[0，1]的列表，其中0指定特殊添加的标记，而1指定序列标记</li>
</ul>
<h3 id="将input-ids转化回token"><a href="#将input-ids转化回token" class="headerlink" title="将input_ids转化回token"></a>将input_ids转化回token</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.convert_ids_to_tokens(sen_code[<span class="string">'input_ids'</span>])</span><br></pre></td></tr></table></figure>
<p>得到的结果是：</p>
<p><code>[&#39;[CLS]&#39;, &#39;i&#39;, &#39;like&#39;, &#39;you&#39;, &#39;[SEP]&#39;, &#39;but&#39;, &#39;not&#39;, &#39;him&#39;, &#39;[SEP]&#39;]</code></p>
<p>即tokenizer在编码时已经默认添加了标记。各模型对应的输入格式是这样的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bert:       [CLS] + tokens + [SEP] + padding</span><br><span class="line"></span><br><span class="line">roberta:    [CLS] + prefix_space + tokens + [SEP] + padding</span><br><span class="line"></span><br><span class="line">distilbert: [CLS] + tokens + [SEP] + padding</span><br><span class="line"></span><br><span class="line">xlm:        [CLS] + tokens + [SEP] + padding</span><br><span class="line"></span><br><span class="line">xlnet:      padding + tokens + [SEP] + [CLS]</span><br></pre></td></tr></table></figure>
<p>其中<code>[CLS]</code>对应分类等任务中的标记，<code>[SEP]</code>对应句子的结束，padding是当指定模型最大输入长度max_len时，需要补充的字符。</p>
<h3 id="对编码进行转换，以便输入Tensor"><a href="#对编码进行转换，以便输入Tensor" class="headerlink" title="对编码进行转换，以便输入Tensor"></a>对编码进行转换，以便输入Tensor</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">model.eval() <span class="comment"># 将模型设为验证模式</span></span><br><span class="line">input_ids = torch.tensor([sen_code[<span class="string">'input_ids'</span>]]) <span class="comment"># 添加batch维度并转化为tensor</span></span><br><span class="line">token_type_ids = torch.tensor([sen_code[<span class="string">'token_type_ids'</span>]])</span><br></pre></td></tr></table></figure>
<h3 id="将分词结果输入模型，得到编码"><a href="#将分词结果输入模型，得到编码" class="headerlink" title="将分词结果输入模型，得到编码"></a>将分词结果输入模型，得到编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将模型转化为eval模式</span></span><br><span class="line">model.eval()</span><br><span class="line"><span class="comment"># 将模型和数据转移到cuda, 若无cuda,可更换为cpu</span></span><br><span class="line">device = <span class="string">'cuda'</span></span><br><span class="line">tokens_tensor = input_ids.to(device)</span><br><span class="line">segments_tensors = token_type_ids.to(device)</span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行编码</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># See the models docstrings for the detail of the inputs</span></span><br><span class="line">    outputs = model(tokens_tensor, token_type_ids=segments_tensors)</span><br><span class="line">    <span class="comment"># Transformers models always output tuples.</span></span><br><span class="line">    <span class="comment"># See the models docstrings for the detail of all the outputs</span></span><br><span class="line">    <span class="comment"># In our case, the first element is the hidden state of the last layer of the Bert model</span></span><br><span class="line">    encoded_layers = outputs</span><br><span class="line"><span class="comment"># 得到最终的编码结果encoded_layers</span></span><br></pre></td></tr></table></figure>
<p>Bert最终输出的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sequence_output, pooled_output, (hidden_states), (attentions)</span><br></pre></td></tr></table></figure>
<p>以输入序列长度为14为例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>index</th>
<th>名称</th>
<th>维度</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>sequence_output</td>
<td>torch.Size([1, 14, 768])</td>
<td>输出序列</td>
</tr>
<tr>
<td>1</td>
<td>pooled_output</td>
<td>torch.Size([1, 768])</td>
<td>对输出序列进行pool操作的结果</td>
</tr>
<tr>
<td>2</td>
<td>(hidden_states)</td>
<td>tuple,13*torch.Size([1, 14, 768])</td>
<td>隐藏层状态(包括Embedding层)，取决于modelconfig中output_hidden_states</td>
</tr>
<tr>
<td>3</td>
<td>(attentions)</td>
<td>tuple,12*torch.Size([1, 12, 14, 14])</td>
<td>注意力层，取决于参数中output_attentions</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Bert总结"><a href="#Bert总结" class="headerlink" title="Bert总结"></a>Bert总结</h3><p>这一节我们以Bert为例对模型整体的流程进行了了解。之后的很多模型都基于Bert，并基于Bert进行了少量的调整。其中的输出和输出参数也有很多重复的地方。</p>
<h2 id="利用预训练模型在下游任务上微调"><a href="#利用预训练模型在下游任务上微调" class="headerlink" title="利用预训练模型在下游任务上微调"></a>利用预训练模型在下游任务上微调</h2><p>如开头所说，这篇文章重点在于”如何进行模型的调整以及输入输出的设定”, 以及”Transformer的实现进行简要的提及”, 所以，我们不会去介绍、涉及如何写train循环等话题，而仅仅专注于模型。也就是说，我们将止步于跑通一个模型，而不计批量数据预处理、训练、验证等过程。</p>
<p>同时，这里更看重如何基于Bert等初始模型在实际任务上进行微调，所以我们不会仅仅地导入已经在下游任务上训练好的模型参数，因为在这些模型上使用的方法和上一章的几乎完全相同。</p>
<p>这里的输入和输入以模型的预测过程为例。</p>
<h3 id="问答任务-via-Bert"><a href="#问答任务-via-Bert" class="headerlink" title="问答任务 via Bert"></a>问答任务 via Bert</h3><p><strong>任务输入</strong>：问题句，答案所在的文章 <code>&quot;Who was Jim Henson?&quot;, &quot;Jim Henson was a nice puppet&quot;</code></p>
<p><strong>任务输出</strong>：答案  <code>&quot;a nice puppet&quot;</code></p>
<p>现存的模型输入输出和任务的输入输出有一定差别，这也是在使用上需要区别的地方：</p>
<p><strong>模型输入</strong>：inputids, token_type_ids</p>
<p><strong>模型输出</strong>：start_scores, end_scores 形状都为<code>torch.Size([1, 14])</code>,其中<code>14</code>为序列长度，代表每个位置是开始/结束位置的概率。</p>
<p><strong>模型的构建</strong>：</p>
<p>一般情况下，一个基本模型对应一个Tokenizer, 所以并不存在对应于具体下游任务的Tokenizer。这里通过bert_model初始化BertForQuestionAnswering。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">MODEL_PATH = <span class="string">r"D:\transformr_files\bert-base-uncased/"</span></span><br><span class="line"><span class="comment"># 实例化tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">r"D:\transformr_files\bert-base-uncased\bert-base-uncased-vocab.txt"</span>)</span><br><span class="line"><span class="comment"># 导入bert的model_config</span></span><br><span class="line">model_config = transformers.BertConfig.from_pretrained(MODEL_PATH)</span><br><span class="line"><span class="comment"># 首先新建bert_model</span></span><br><span class="line">bert_model = transformers.BertModel.from_pretrained(MODEL_PATH,config = model_config)</span><br><span class="line"><span class="comment"># 最终有两个输出，初始位置和结束位置（下面有解释）</span></span><br><span class="line">model_config.num_labels = <span class="number">2</span></span><br><span class="line"><span class="comment"># 同样根据bert的model_config新建BertForQuestionAnswering</span></span><br><span class="line">model = BertForQuestionAnswering(model_config)</span><br><span class="line">model.bert = bert_model</span><br></pre></td></tr></table></figure>
<p><strong>利用模型进行运算：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定模式</span></span><br><span class="line">model.eval()</span><br><span class="line">question, text = <span class="string">"Who was Jim Henson?"</span>, <span class="string">"Jim Henson was a nice puppet"</span></span><br><span class="line"><span class="comment"># 获取input_ids编码</span></span><br><span class="line">input_ids = tokenizer.encode(question, text)</span><br><span class="line"><span class="comment"># 手动进行token_type_ids编码，可用encode_plus代替</span></span><br><span class="line">token_type_ids = [<span class="number">0</span> <span class="keyword">if</span> i &lt;= input_ids.index(<span class="number">102</span>) <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(len(input_ids))]</span><br><span class="line"><span class="comment"># 得到评分, </span></span><br><span class="line">start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))</span><br><span class="line"><span class="comment"># 进行逆编码，得到原始的token </span></span><br><span class="line">all_tokens = tokenizer.convert_ids_to_tokens(input_ids)</span><br><span class="line"><span class="comment">#['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', 'henson', 'was', 'a', 'nice', 'puppet', '[SEP]']</span></span><br></pre></td></tr></table></figure>
<p><strong>将模型输出转化为任务输出：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对输出的答案进行解码的过程</span></span><br><span class="line">answer = <span class="string">' '</span>.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+<span class="number">1</span>])</span><br><span class="line"><span class="comment"># assert answer == "a nice puppet" </span></span><br><span class="line"><span class="comment"># 这里因为没有经过微调，所以效果不是很好，输出结果不佳。</span></span><br><span class="line">print(answer)</span><br><span class="line"><span class="comment"># 'was jim henson ? [SEP] jim henson was a nice puppet [SEP]'</span></span><br></pre></td></tr></table></figure>
<h3 id="文本分类任务-情感分析等-via-XLNet"><a href="#文本分类任务-情感分析等-via-XLNet" class="headerlink" title="文本分类任务(情感分析等) via XLNet"></a>文本分类任务(情感分析等) via XLNet</h3><p><strong>任务输入</strong>：句子 <code>&quot;i like you, what about you&quot;</code></p>
<p><strong>任务输出</strong>：句子所属的类别 <code>class1</code></p>
<p><strong>模型输入</strong>：inputids, token_type_ids</p>
<p><strong>模型输出</strong>：logits, hidden states， 其中logits形状为<code>torch.Size([1, 3])</code>, 其中的3对应的是类别的数量。当训练时，第一项为loss。</p>
<p><strong>模型的构建</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetConfig, XLNetModel, XLNetTokenizer, XLNetForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># 定义路径，初始化tokenizer</span></span><br><span class="line">XLN_PATH = <span class="string">r"D:\transformr_files\XLNetLMHeadModel"</span></span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(XLN_PATH)</span><br><span class="line"><span class="comment"># 加载配置</span></span><br><span class="line">model_config = XLNetConfig.from_pretrained(XLN_PATH)</span><br><span class="line"><span class="comment"># 设定类别数为3</span></span><br><span class="line">model_config.num_labels = <span class="number">3</span></span><br><span class="line"><span class="comment"># 直接从xlnet的config新建XLNetForSequenceClassification(和上一节方法等效)</span></span><br><span class="line">cls_model = XLNetForSequenceClassification.from_pretrained(XLN_PATH, config=model_config)</span><br></pre></td></tr></table></figure>
<p><strong>利用模型进行运算：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设定模式</span></span><br><span class="line">model.eval()</span><br><span class="line">token_codes = tokenizer.encode_plus(<span class="string">"i like you, what about you"</span>)</span><br><span class="line"><span class="comment"># encode_plus结果为字典形式</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = cls_model(input_ids=torch.tensor([token_codes[<span class="string">'input_ids'</span>]]),token_type_ids = torch.tensor([token_codes[<span class="string">'token_type_ids'</span>]]))</span><br><span class="line"><span class="comment"># outputs[0]为logits，outputs[1]为hidden</span></span><br></pre></td></tr></table></figure>
<p>输出的转化可直接通过numpy的argmax函数实现。</p>
<h3 id="其他的任务，将继续更新"><a href="#其他的任务，将继续更新" class="headerlink" title="其他的任务，将继续更新"></a>其他的任务，将继续更新</h3><p>其他的模型和之前的两个大致是相同的，你可以自己发挥。我会继续在相关的库上进行实验，如果发现用法不一样的情况，将会添加在这里。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>本文章主要对HuggingFace库进行了简要介绍。具体安装等过程请参见官方<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener">github仓库</a>。</p>
<p>本文主要参考于<a href="https://huggingface.co/transformers/installation.html#" target="_blank" rel="noopener">官方文档</a></p>
<p>同时，在模型的理解过程中参考了一些kaggle上的notebooks, <a href="https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch" target="_blank" rel="noopener">主要是这一篇</a>，作者是<a href="https://www.kaggle.com/abhishek" target="_blank" rel="noopener">Abhishek Thakur</a></p>
<h1 id="修改记录"><a href="#修改记录" class="headerlink" title="修改记录"></a>修改记录</h1><ul>
<li>2020/5/4 <ul>
<li>添加不同模型需要的分词格式变化</li>
<li>增改论文链接</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/29/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/29/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/" class="post-title-link" itemprop="url">Bert系列伴生的新分词器</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-29 09:31:52" itemprop="dateCreated datePublished" datetime="2020-04-29T09:31:52+08:00">2020-04-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-04 09:53:09" itemprop="dateModified" datetime="2020-05-04T09:53:09+08:00">2020-05-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概括"><a href="#概括" class="headerlink" title="概括"></a>概括</h2><p>这篇文章将对Bert等模型使用的分词技术进行介绍。同时会涉及这些分词器在huggingface <a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">tokenizers</a>库中的使用。理解这些分词器的原理，对于灵活使用transformers库中的不同模型非常重要。除此之外，我们还能将这些分词器用于其他任务中，如果有必要的话，我们还能自己训练分词器。</p>
<h2 id="分词器是做什么的？"><a href="#分词器是做什么的？" class="headerlink" title="分词器是做什么的？"></a>分词器是做什么的？</h2><p><strong>机器无法理解文本。</strong>当我们将句子序列送入模型时，模型仅仅能看到一串字节，它无法知道一个词从哪里开始，到哪里结束，所以也不知道一个词是怎么组成的。</p>
<p>​    所以，为了帮助机器理解文本，我们需要</p>
<ol>
<li>将文本分成一个个小片段</li>
<li>然后将这些片段表示为一个向量作为模型的输入</li>
<li>同时，我们需要将一个个小片段（token) 表示为向量，作为词嵌入矩阵， 通过在语料库上训练来优化token的表示，使其蕴含更多有用的信息，用于之后的任务。</li>
</ol>
<h2 id="古典分词方法"><a href="#古典分词方法" class="headerlink" title="古典分词方法"></a>古典分词方法</h2><p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/tokenize.png" alt="tokenize"></p>
<h3 id="基于空格的分词方法"><a href="#基于空格的分词方法" class="headerlink" title="基于空格的分词方法"></a>基于空格的分词方法</h3><p>一个句子，使用不同的规则，将有许多种不同的分词结果。我们之前常用的分词方法将空格作为分词的边界。也就是图中的第三种方法。但是，这种方法存在问题，即只有在训练语料中出现的token才能被训练器学习到，而那些没有出现的token将会被<code>&lt;UNK&gt;</code>等特殊标记代替，这样将影响模型的表现。如果我们将词典做得足够大，使其能容纳所有的单词。那么词典将非常庞大，产生很大的开销。同时对于出现次数很少的词，学习其token的向量表示也非常困难。除去这些原因，有很多语言不用空格进行分词，也就无法使用基于空格分词的方法。综上，我们需要新的分词方法来解决这些问题。</p>
<h3 id="基于字母的分词方法"><a href="#基于字母的分词方法" class="headerlink" title="基于字母的分词方法"></a>基于字母的分词方法</h3><p>简单来说，就是将每个字符看作一个词。</p>
<p><strong>优点</strong>： 不用担心未知词汇，可以为每一个单词生成词嵌入向量表示。</p>
<p><strong>缺点</strong>：</p>
<ul>
<li>字母本身就没有任何的内在含义，得到的词嵌入向量缺乏含义。</li>
<li>计算复杂度提升（字母的数目远大于token的数目）</li>
<li>输出序列的长度将变大，对于Bert、CNN等限制最大长度的模型将很容易达到最大值。</li>
</ul>
<h2 id="基于子词的分词方法（Subword-Tokenization）"><a href="#基于子词的分词方法（Subword-Tokenization）" class="headerlink" title="基于子词的分词方法（Subword Tokenization）"></a>基于子词的分词方法（Subword Tokenization）</h2><p>为了改进分词方法，在<code>&lt;UNK&gt;</code>数目和词向量含义丰富性之间达到平衡，提出了Subword Tokenization方法。这种方法的目的是通过一个有限的单词列表来解决所有单词的分词问题，同时将结果中token的数目降到最低。例如，可以用更小的词片段来组成更大的词：</p>
<p>“<strong><em>unfortunately</em></strong>” = “<strong><em>un</em></strong>” + “<strong><em>for</em></strong>” + “<strong><em>tun</em></strong>” + “<strong><em>ate</em></strong>” + “<strong><em>ly</em></strong>”。</p>
<p>接下来，将介绍几种不同的Subword Tokenization方法。</p>
<h3 id="Byte-Pair-Encoding-BPE-字节对编码"><a href="#Byte-Pair-Encoding-BPE-字节对编码" class="headerlink" title="Byte Pair Encoding (BPE) 字节对编码"></a>Byte Pair Encoding (BPE) 字节对编码</h3><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><p>字节对编码最早是在信号压缩领域提出的，后来被应用于分词任务中。在信号压缩领域中BPE过程可视化如下：</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/1_x1Y_n3sXGygUPSdfXTm9pQ.gif" alt="1_x1Y_n3sXGygUPSdfXTm9pQ"></p>
<p>接下来重点介绍将BPE应用于分词任务的流程：</p>
<h3 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h3><ol>
<li>根据语料库建立一个词典，词典中仅包含单个字符，如英文中就是a-z</li>
<li>统计语料库中出现次数最多的字符对（词典中两项的组合），然后将字符对加入到词典中</li>
<li>重复步骤2直到到达规定的步骤数目或者词典尺寸缩小到了指定的值。</li>
</ol>
<h3 id="BPE的优点"><a href="#BPE的优点" class="headerlink" title="BPE的优点"></a>BPE的优点</h3><p>可以很有效地平衡词典尺寸和编码步骤数(将句子编码所需要的token数量)</p>
<h3 id="BPE存在的缺点："><a href="#BPE存在的缺点：" class="headerlink" title="BPE存在的缺点："></a><strong>BPE</strong>存在的缺点：</h3><p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/image-20200429104839759.png" alt="image-20200429104839759"></p>
<ul>
<li>对于同一个句子, 例如Hello world，如图所示，可能会有不同的Subword序列。不同的Subword序列会产生完全不同的id序列表示，这种歧义可能在解码阶段无法解决。在翻译任务中，不同的id序列可能翻译出不同的句子，这显然是错误的。</li>
<li>在训练任务中，如果能对不同的Subword进行训练的话，将增加模型的健壮性，能够容忍更多的噪声，而BPE的贪心算法无法对随机分布进行学习。</li>
</ul>
<h3 id="Unigram-Based-Tokenization"><a href="#Unigram-Based-Tokenization" class="headerlink" title="Unigram Based Tokenization"></a>Unigram Based Tokenization</h3><h3 id="方法概述"><a href="#方法概述" class="headerlink" title="方法概述"></a>方法概述</h3><p>分词中的Unigram模型是<strong>Kudo.</strong>在论文<strong>“Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates”</strong>中提出的。当时主要是为了解决机器翻译中分词的问题。作者使用一种叫做<code>marginalized likelihood</code>的方法来建模翻译问题，考虑到不同分词结果对最终翻译结果的影响，引入了分词概率$P(\vec{x}|X)$来表示$X$最终分词为$\vec{x}$的概率(X为原始的句子, $\vec{x}$为分词的结果$\vec{x} = (x_1, . . . , x_M) $，由多个subword组成)。传统的BPE算法无法对这个概率进行建模，因此作者使用了Unigram语言模型来达到这样的目的。</p>
<h3 id="方法执行过程"><a href="#方法执行过程" class="headerlink" title="方法执行过程"></a>方法执行过程</h3><p><strong>假设</strong>：根据unigram的假设，每个字词的出现是独立的。所以</p>
<script type="math/tex; mode=display">
P(\vec{x}) = \prod_{i=1}^{M}p(x_i)</script><p>这里的$x_i$是从预先定义好的词典$V$中取得的，所以，最有可能的分词方式就可以这样表示：</p>
<script type="math/tex; mode=display">
x^* =\underset{x\in S(X)}{arg\;max}\;P(\vec{x})</script><p>这里$S(X)$是句子$X$不同的分词结果集合。$x^*$可以通过维特比算法得到。</p>
<p>如果已知词典$V$, 我们可以通过EM算法来估计$p(x_i)$，其中M步最大化的对象是以下似然函数（原谅我这里偷懒直接使用图片）：</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/image-20200501185312612.png" alt="image-20200501185312612"></p>
<p>$|D|$是语料库中语料数量。</p>
<p><strong>我是这样理解这个似然函数的：</strong>将语料库中所有句子的所有分词组合形成的概率相加。</p>
<p>初始时，我们连词典$V$都没有，作者通过不断执行以下步骤来构造合适的词典以及分词概率：</p>
<ol>
<li><p>从头构建一个相当大的种子词典。</p>
</li>
<li><p>重复以下步骤，知道字典尺寸$|V|$减小到期望值：</p>
<ul>
<li><p>固定词典，通过EM算法优化$p(x)$</p>
</li>
<li><p>为每一个子词计算$loss_i$，loss代表如果将某个词去掉，上述似然函数值会减少多少。根据loss排序，保留loss最高的$\eta$个子词。注意：保留所有的单字符，从而避免OOV情况。</p>
<p><strong>我是这样理解loss的：</strong>若某个子词经常以很高的频率出现在很多句子的分词结果中，那么其损失将会很大，所以要保留这样的子词。</p>
</li>
</ul>
</li>
</ol>
<h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><ol>
<li>使用的训练算法可以利用所有可能的分词结果，这是通过data sampling算法实现的。</li>
<li>提出一种基于语言模型的分词算法，这种语言模型可以给多种分词结果赋予概率，从而可以学得其中的噪声。</li>
</ol>
<h2 id="将基于子词的分词方法应用到实际中"><a href="#将基于子词的分词方法应用到实际中" class="headerlink" title="将基于子词的分词方法应用到实际中"></a>将基于子词的分词方法应用到实际中</h2><h3 id="Bert中的WordPiece分词器"><a href="#Bert中的WordPiece分词器" class="headerlink" title="Bert中的WordPiece分词器"></a>Bert中的WordPiece分词器</h3><p>WordPiece是随着Bert论文的出现被提出的。在整体步骤上，WordPiece方法和BPE是相同的。即也是自低向上地构建词典。区别是BPE在每次合并的时候都选择出现次数最高的字符对，而WordPiece使用的是类似于Unigram的方法，即通过语言模型来得到合并两个单词可能造成的影响，然后选择使得似然函数提升最大的字符对。这个提升是通过结合后的字符对减去结合前的字符对之和得到的。也就是说，判断“de”相较于“d”+”e”是否更适合出现。</p>
<p>三种分词器的关系如下：(图自<a href="https://blog.floydhub.com/tokenization-nlp/" target="_blank" rel="noopener">FloudHub Blog</a>)</p>
<p><img src="/images/Bert%E7%B3%BB%E5%88%97%E4%BC%B4%E7%94%9F%E7%9A%84%E6%96%B0%E5%88%86%E8%AF%8D%E5%99%A8/subword-probabilistic-tokenization.png" alt="Frequency V probability approaches"></p>
<h3 id="SentencePiece库"><a href="#SentencePiece库" class="headerlink" title="SentencePiece库"></a>SentencePiece库</h3><p>SentencePiece是在“SentencePiece: A simple and language independent subword tokenizer<br>and detokenizer for Neural Text Processing”这篇文章中介绍的。其主要是为了解决不同语言分词规则需要特别定义的问题，比如下面这种情况：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Raw text: Hello world.</span><br><span class="line">Tokenized: [Hello] [world] [.]</span><br><span class="line">Decoded text: Hello world .</span><br></pre></td></tr></table></figure>
<p>将分词结果解码到原来的句子中时，会在不同的词之间添加空格，生成<code>Decoded text</code>所示的结果，这就是编码解码出现的歧义性，因此需要特别定义规则来实现互逆。还有一个例子是，在解码阶段，欧洲语言词之间要添加空格，而中文等语言则不应添加空格。对于这种区别，也需要单独定制规则，这些繁杂的规则维护起来非常困难，所以作者采用以下的方案来解决：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将所有的字符都转化成Unicode编码，空格用‘_’来代替，然后进行分词操作。这样空格也不需要特别定义规则了。然后在解码结束后，使用Python代码恢复即可：</span><br><span class="line">detok = ’’.join(tokens).replace(’_’, ’ ’)</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece库</a>主要由以下部分组成：</p>
<p><strong>“Normalizer, Trainer, Encoder,  Decoder”</strong></p>
<p>其中Normalizer用来对Unicode编码进行规范化，这里使用的算法是<code>NFKC</code>方法，同时也支持自定义规范化方法。Trainer则用来训练分词模型。Encoder是将句子变成编码，而Decoder是反向操作。他们之间存在以下函数关系：</p>
<script type="math/tex; mode=display">
Decode(Encode(Normalize(text))) = Normalize(text):</script><h3 id="Huggingface-tokenizers库的介绍和使用"><a href="#Huggingface-tokenizers库的介绍和使用" class="headerlink" title="Huggingface tokenizers库的介绍和使用"></a>Huggingface tokenizers库的介绍和使用</h3><p><a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">tokenizers</a>是集合了当前最常用的分词器集合，效率和易用性也是其关注的范畴。</p>
<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenizers provides ultra-fast implementations of most current tokenizers:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> tokenizers <span class="keyword">import</span> (ByteLevelBPETokenizer,</span><br><span class="line">                            CharBPETokenizer,</span><br><span class="line">                            SentencePieceBPETokenizer,</span><br><span class="line">                            BertWordPieceTokenizer)</span><br><span class="line"><span class="comment"># Ultra-fast =&gt; they can encode 1GB of text in ~20sec on a standard server's CPU</span></span><br><span class="line"><span class="comment"># Tokenizers can be easily instantiated from standard files</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertWordPieceTokenizer(<span class="string">"bert-base-uncased-vocab.txt"</span>, lowercase=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenizers provide exhaustive outputs: tokens, mapping to original string, attention/special token masks.</span></span><br><span class="line"><span class="comment"># They also handle model's max input lengths as well as padding (to directly encode in padded batches)</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = tokenizer.encode(<span class="string">"Hello, y'all! How are you 😁 ?"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(output.ids, output.tokens, output.offsets)</span><br><span class="line">[<span class="number">101</span>, <span class="number">7592</span>, <span class="number">1010</span>, <span class="number">1061</span>, <span class="number">1005</span>, <span class="number">2035</span>, <span class="number">999</span>, <span class="number">2129</span>, <span class="number">2024</span>, <span class="number">2017</span>, <span class="number">100</span>, <span class="number">1029</span>, <span class="number">102</span>]</span><br><span class="line">[<span class="string">'[CLS]'</span>, <span class="string">'hello'</span>, <span class="string">','</span>, <span class="string">'y'</span>, <span class="string">"'"</span>, <span class="string">'all'</span>, <span class="string">'!'</span>, <span class="string">'how'</span>, <span class="string">'are'</span>, <span class="string">'you'</span>, <span class="string">'[UNK]'</span>, <span class="string">'?'</span>, <span class="string">'[SEP]'</span>]</span><br><span class="line">[(<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">5</span>), (<span class="number">5</span>, <span class="number">6</span>), (<span class="number">7</span>, <span class="number">8</span>), (<span class="number">8</span>, <span class="number">9</span>), (<span class="number">9</span>, <span class="number">12</span>), (<span class="number">12</span>, <span class="number">13</span>), (<span class="number">14</span>, <span class="number">17</span>), (<span class="number">18</span>, <span class="number">21</span>), (<span class="number">22</span>, <span class="number">25</span>), (<span class="number">26</span>, <span class="number">27</span>),(<span class="number">28</span>, <span class="number">29</span>), (<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line"><span class="comment"># Here is an example using the offsets mapping to retrieve the string corresponding to the 10th token:</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.original_str[output.offsets[<span class="number">10</span>]]</span><br><span class="line"><span class="string">'😁'</span></span><br></pre></td></tr></table></figure>
<h3 id="自己训练分词器"><a href="#自己训练分词器" class="headerlink" title="自己训练分词器"></a>自己训练分词器</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># You can also train a BPE/Byte-levelBPE/WordPiece vocabulary on your own files</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = ByteLevelBPETokenizer()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.train([<span class="string">"wiki.test.raw"</span>], vocab_size=<span class="number">20000</span>)</span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>] Tokenize words                 ████████████████████████████████████████   <span class="number">20993</span>/<span class="number">20993</span></span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span>] Count pairs                    ████████████████████████████████████████   <span class="number">20993</span>/<span class="number">20993</span></span><br><span class="line">[<span class="number">00</span>:<span class="number">00</span>:<span class="number">03</span>] Compute merges                 ████████████████████████████████████████   <span class="number">19375</span>/<span class="number">19375</span></span><br></pre></td></tr></table></figure>
<h2 id="参考材料"><a href="#参考材料" class="headerlink" title="参考材料"></a>参考材料</h2><p>这篇文章是在Floydhub的<a href="https://blog.floydhub.com/tokenization-nlp/" target="_blank" rel="noopener">一篇博客</a>基础上扩展的。还主要参考了Unigram的原论文，BPE的官方解释等。BPE的动态图来自于Toward data science的<a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" target="_blank" rel="noopener">有关博客</a>。除此之外，最后一章参考于tokenizers的<a href="https://github.com/huggingface/tokenizers" target="_blank" rel="noopener">官方仓库</a>。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0notebook%E5%A4%9A%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%A0%B7%E6%9C%AC%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/27/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0notebook%E5%A4%9A%E6%8A%98%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%A0%B7%E6%9C%AC%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">深度学习Notebook多折交叉验证样本代码</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-27 11:45:20 / 修改时间：12:17:16" itemprop="dateCreated datePublished" datetime="2020-04-27T11:45:20+08:00">2020-04-27</time>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="深度学习notebook多折交叉验证样本代码"><a href="#深度学习notebook多折交叉验证样本代码" class="headerlink" title="深度学习notebook多折交叉验证样本代码"></a>深度学习notebook多折交叉验证样本代码</h1><p><strong>这份样本代码基于 @abhishek’s <a href="https://www.kaggle.com/abhishek/bert-base-uncased-using-pytorch" target="_blank" rel="noopener">BERT Base Uncased using PyTorch</a>在tweet情感词抽取比赛上的notebook</strong>, 如果决定使用代码，请为abhishek投上一个赞成票。</p>
<h1 id="导入需要用到的包"><a href="#导入需要用到的包" class="headerlink" title="导入需要用到的包"></a>导入需要用到的包</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="comment"># tqdm用来记录notebook训练的进度条</span></span><br><span class="line"><span class="keyword">from</span> tqdm.autonotebook <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">import</span> tokenizers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AdamW</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> get_linear_schedule_with_warmup</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">config</span>:</span></span><br><span class="line">    MAX_LEN = <span class="number">128</span></span><br><span class="line">    TRAIN_BATCH_SIZE = <span class="number">64</span></span><br><span class="line">    VALID_BATCH_SIZE = <span class="number">16</span></span><br><span class="line">    EPOCHS = <span class="number">6</span></span><br><span class="line">    ROBERTA_PATH = <span class="string">"../input/roberta-base/"</span></span><br><span class="line">    MODEL_PATH = <span class="string">"pytorch_model.bin"</span></span><br><span class="line">    TRAINING_FILE = <span class="string">"../input/tweet-train-folds/train_folds.csv"</span></span><br><span class="line">    TOKENIZER = tokenizers.ByteLevelBPETokenizer(</span><br><span class="line">    vocab_file=<span class="string">f"<span class="subst">&#123;ROBERTA_PATH&#125;</span>/vocab.json"</span>, </span><br><span class="line">    merges_file=<span class="string">f"<span class="subst">&#123;ROBERTA_PATH&#125;</span>/merges.txt"</span>, </span><br><span class="line">    lowercase=<span class="literal">True</span>,</span><br><span class="line">    add_prefix_space=<span class="literal">True</span></span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<h1 id="Data-Processing"><a href="#Data-Processing" class="headerlink" title="Data Processing"></a>Data Processing</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_data</span><span class="params">(tweet, selected_text, sentiment, tokenizer, max_len)</span>:</span></span><br><span class="line">    tweet = <span class="string">" "</span> + <span class="string">" "</span>.join(str(tweet).split())</span><br><span class="line">    selected_text = <span class="string">" "</span> + <span class="string">" "</span>.join(str(selected_text).split())</span><br><span class="line"></span><br><span class="line">    len_st = len(selected_text) - <span class="number">1</span></span><br><span class="line">    idx0 = <span class="literal">None</span></span><br><span class="line">    idx1 = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> (i <span class="keyword">for</span> i, e <span class="keyword">in</span> enumerate(tweet) <span class="keyword">if</span> e == selected_text[<span class="number">1</span>]):</span><br><span class="line">        <span class="keyword">if</span> <span class="string">" "</span> + tweet[ind: ind+len_st] == selected_text:</span><br><span class="line">            idx0 = ind</span><br><span class="line">            idx1 = ind + len_st - <span class="number">1</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    char_targets = [<span class="number">0</span>] * len(tweet)</span><br><span class="line">    <span class="keyword">if</span> idx0 != <span class="literal">None</span> <span class="keyword">and</span> idx1 != <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">for</span> ct <span class="keyword">in</span> range(idx0, idx1 + <span class="number">1</span>):</span><br><span class="line">            char_targets[ct] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    tok_tweet = tokenizer.encode(tweet)</span><br><span class="line">    input_ids_orig = tok_tweet.ids</span><br><span class="line">    tweet_offsets = tok_tweet.offsets</span><br><span class="line">    </span><br><span class="line">    target_idx = []</span><br><span class="line">    <span class="keyword">for</span> j, (offset1, offset2) <span class="keyword">in</span> enumerate(tweet_offsets):</span><br><span class="line">        <span class="keyword">if</span> sum(char_targets[offset1: offset2]) &gt; <span class="number">0</span>:</span><br><span class="line">            target_idx.append(j)</span><br><span class="line">    </span><br><span class="line">    targets_start = target_idx[<span class="number">0</span>]</span><br><span class="line">    targets_end = target_idx[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    sentiment_id = &#123;</span><br><span class="line">        <span class="string">'positive'</span>: <span class="number">1313</span>,</span><br><span class="line">        <span class="string">'negative'</span>: <span class="number">2430</span>,</span><br><span class="line">        <span class="string">'neutral'</span>: <span class="number">7974</span></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    input_ids = [<span class="number">0</span>] + [sentiment_id[sentiment]] + [<span class="number">2</span>] + [<span class="number">2</span>] + input_ids_orig + [<span class="number">2</span>]</span><br><span class="line">    token_type_ids = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>] + [<span class="number">0</span>] * (len(input_ids_orig) + <span class="number">1</span>)</span><br><span class="line">    mask = [<span class="number">1</span>] * len(token_type_ids)</span><br><span class="line">    tweet_offsets = [(<span class="number">0</span>, <span class="number">0</span>)] * <span class="number">4</span> + tweet_offsets + [(<span class="number">0</span>, <span class="number">0</span>)]</span><br><span class="line">    targets_start += <span class="number">4</span></span><br><span class="line">    targets_end += <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    padding_length = max_len - len(input_ids)</span><br><span class="line">    <span class="keyword">if</span> padding_length &gt; <span class="number">0</span>:</span><br><span class="line">        input_ids = input_ids + ([<span class="number">1</span>] * padding_length)</span><br><span class="line">        mask = mask + ([<span class="number">0</span>] * padding_length)</span><br><span class="line">        token_type_ids = token_type_ids + ([<span class="number">0</span>] * padding_length)</span><br><span class="line">        tweet_offsets = tweet_offsets + ([(<span class="number">0</span>, <span class="number">0</span>)] * padding_length)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">'ids'</span>: input_ids,</span><br><span class="line">        <span class="string">'mask'</span>: mask,</span><br><span class="line">        <span class="string">'token_type_ids'</span>: token_type_ids,</span><br><span class="line">        <span class="string">'targets_start'</span>: targets_start,</span><br><span class="line">        <span class="string">'targets_end'</span>: targets_end,</span><br><span class="line">        <span class="string">'orig_tweet'</span>: tweet,</span><br><span class="line">        <span class="string">'orig_selected'</span>: selected_text,</span><br><span class="line">        <span class="string">'sentiment'</span>: sentiment,</span><br><span class="line">        <span class="string">'offsets'</span>: tweet_offsets</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h1 id="Data-loader"><a href="#Data-loader" class="headerlink" title="Data loader"></a>Data loader</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TweetDataset</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Dataset which stores the tweets and returns them as processed features</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tweet, sentiment, selected_text)</span>:</span></span><br><span class="line">        self.tweet = tweet</span><br><span class="line">        self.sentiment = sentiment</span><br><span class="line">        self.selected_text = selected_text</span><br><span class="line">        self.tokenizer = config.TOKENIZER</span><br><span class="line">        self.max_len = config.MAX_LEN</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.tweet)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, item)</span>:</span></span><br><span class="line">        data = process_data(</span><br><span class="line">            self.tweet[item], </span><br><span class="line">            self.selected_text[item], </span><br><span class="line">            self.sentiment[item],</span><br><span class="line">            self.tokenizer,</span><br><span class="line">            self.max_len</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Return the processed data where the lists are converted to `torch.tensor`s</span></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">'ids'</span>: torch.tensor(data[<span class="string">"ids"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'mask'</span>: torch.tensor(data[<span class="string">"mask"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'token_type_ids'</span>: torch.tensor(data[<span class="string">"token_type_ids"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'targets_start'</span>: torch.tensor(data[<span class="string">"targets_start"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'targets_end'</span>: torch.tensor(data[<span class="string">"targets_end"</span>], dtype=torch.long),</span><br><span class="line">            <span class="string">'orig_tweet'</span>: data[<span class="string">"orig_tweet"</span>],</span><br><span class="line">            <span class="string">'orig_selected'</span>: data[<span class="string">"orig_selected"</span>],</span><br><span class="line">            <span class="string">'sentiment'</span>: data[<span class="string">"sentiment"</span>],</span><br><span class="line">            <span class="string">'offsets'</span>: torch.tensor(data[<span class="string">"offsets"</span>], dtype=torch.long)</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<h1 id="模型定义"><a href="#模型定义" class="headerlink" title="模型定义"></a>模型定义</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TweetModel</span><span class="params">(transformers.BertPreTrainedModel)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, conf)</span>:</span></span><br><span class="line">        super(TweetModel, self).__init__(conf)</span><br><span class="line">        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=conf)</span><br><span class="line">        self.drop_out = nn.Dropout(<span class="number">0.1</span>)</span><br><span class="line">        self.l_1 = nn.Linear(<span class="number">768</span> * <span class="number">2</span>, <span class="number">400</span>)</span><br><span class="line">        self.l0 = nn.Linear(<span class="number">400</span>, <span class="number">2</span>)</span><br><span class="line">        torch.nn.init.normal_(self.l0.weight, std=<span class="number">0.02</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ids, mask, token_type_ids)</span>:</span></span><br><span class="line">        _, _, out = self.roberta(</span><br><span class="line">            ids,</span><br><span class="line">            attention_mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        out = torch.cat((out[<span class="number">-1</span>], out[<span class="number">-2</span>]), dim=<span class="number">-1</span>)</span><br><span class="line">        out = self.drop_out(out)</span><br><span class="line">        logits = self.l_1(out)</span><br><span class="line">        logits = self.l0(logits)</span><br><span class="line"></span><br><span class="line">        start_logits, end_logits = logits.split(<span class="number">1</span>, dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        start_logits = start_logits.squeeze(<span class="number">-1</span>)</span><br><span class="line">        end_logits = end_logits.squeeze(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> start_logits, end_logits</span><br></pre></td></tr></table></figure>
<h1 id="自定义损失函数（optional-取决于任务）"><a href="#自定义损失函数（optional-取决于任务）" class="headerlink" title="自定义损失函数（optional,取决于任务）"></a>自定义损失函数（optional,取决于任务）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_fn</span><span class="params">(start_logits, end_logits, start_positions, end_positions)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Return the sum of the cross entropy losses for both the start and end logits</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss_fct = nn.CrossEntropyLoss()</span><br><span class="line">    start_loss = loss_fct(start_logits, start_positions)</span><br><span class="line">    end_loss = loss_fct(end_logits, end_positions)</span><br><span class="line">    total_loss = (start_loss + end_loss)</span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br></pre></td></tr></table></figure>
<h1 id="Training-Function"><a href="#Training-Function" class="headerlink" title="Training Function"></a>Training Function</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_fn</span><span class="params">(data_loader, model, optimizer, device, scheduler=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Trains the bert model on the twitter data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Set model to training mode (dropout + sampled batchnorm is activated)</span></span><br><span class="line">    model.train()</span><br><span class="line">    losses = utils.AverageMeter()</span><br><span class="line">    jaccards = utils.AverageMeter()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set tqdm to add loading screen and set the length</span></span><br><span class="line">    tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Train the model on each batch</span></span><br><span class="line">    <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line"></span><br><span class="line">        ids = d[<span class="string">"ids"</span>]</span><br><span class="line">        token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">        mask = d[<span class="string">"mask"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">        orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">        orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        offsets = d[<span class="string">"offsets"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Move ids, masks, and targets to gpu while setting as torch.long</span></span><br><span class="line">        ids = ids.to(device, dtype=torch.long)</span><br><span class="line">        token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">        mask = mask.to(device, dtype=torch.long)</span><br><span class="line">        targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">        targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Reset gradients</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        <span class="comment"># Use ids, masks, and token types as input to the model</span></span><br><span class="line">        <span class="comment"># Predict logits for each of the input tokens for each batch</span></span><br><span class="line">        outputs_start, outputs_end = model(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">        ) <span class="comment"># (bs x SL), (bs x SL)</span></span><br><span class="line">        <span class="comment"># Calculate batch loss based on CrossEntropy</span></span><br><span class="line">        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)</span><br><span class="line">        <span class="comment"># Calculate gradients based on loss</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># Adjust weights based on calculated gradients</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment"># Update scheduler</span></span><br><span class="line">        scheduler.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Apply softmax to the start and end logits</span></span><br><span class="line">        <span class="comment"># This squeezes each of the logits in a sequence to a value between 0 and 1, while ensuring that they sum to 1</span></span><br><span class="line">        <span class="comment"># This is similar to the characteristics of "probabilities"</span></span><br><span class="line">        outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Calculate the jaccard score based on the predictions for this batch</span></span><br><span class="line">        jaccard_scores = []</span><br><span class="line">        <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">            selected_tweet = orig_selected[px]</span><br><span class="line">            tweet_sentiment = sentiment[px]</span><br><span class="line">            jaccard_score, _ = calculate_jaccard_score(</span><br><span class="line">                original_tweet=tweet, <span class="comment"># Full text of the px'th tweet in the batch</span></span><br><span class="line">                target_string=selected_tweet, <span class="comment"># Span containing the specified sentiment for the px'th tweet in the batch</span></span><br><span class="line">                sentiment_val=tweet_sentiment, <span class="comment"># Sentiment of the px'th tweet in the batch</span></span><br><span class="line">                idx_start=np.argmax(outputs_start[px, :]), <span class="comment"># Predicted start index for the px'th tweet in the batch</span></span><br><span class="line">                idx_end=np.argmax(outputs_end[px, :]), <span class="comment"># Predicted end index for the px'th tweet in the batch</span></span><br><span class="line">                offsets=offsets[px] <span class="comment"># Offsets for each of the tokens for the px'th tweet in the batch</span></span><br><span class="line">            )</span><br><span class="line">            jaccard_scores.append(jaccard_score)</span><br><span class="line">        <span class="comment"># Update the jaccard score and loss</span></span><br><span class="line">        <span class="comment"># For details, refer to `AverageMeter` in https://www.kaggle.com/abhishek/utils</span></span><br><span class="line">        jaccards.update(np.mean(jaccard_scores), ids.size(<span class="number">0</span>))</span><br><span class="line">        losses.update(loss.item(), ids.size(<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># Print the average loss and jaccard score at the end of each batch</span></span><br><span class="line">        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)</span><br></pre></td></tr></table></figure>
<h1 id="Evaluation-Functions"><a href="#Evaluation-Functions" class="headerlink" title="Evaluation Functions"></a>Evaluation Functions</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_jaccard_score</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    original_tweet, </span></span></span><br><span class="line"><span class="function"><span class="params">    target_string, </span></span></span><br><span class="line"><span class="function"><span class="params">    sentiment_val, </span></span></span><br><span class="line"><span class="function"><span class="params">    idx_start, </span></span></span><br><span class="line"><span class="function"><span class="params">    idx_end, </span></span></span><br><span class="line"><span class="function"><span class="params">    offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">    verbose=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># A span's start index has to be greater than or equal to the end index</span></span><br><span class="line">    <span class="comment"># If this doesn't hold, the start index is set to equal the end index (the span is a single token)</span></span><br><span class="line">    <span class="keyword">if</span> idx_end &lt; idx_start:</span><br><span class="line">        idx_end = idx_start</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Combine into a string the tokens that belong to the predicted span</span></span><br><span class="line">    filtered_output  = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> ix <span class="keyword">in</span> range(idx_start, idx_end + <span class="number">1</span>):</span><br><span class="line">        filtered_output += original_tweet[offsets[ix][<span class="number">0</span>]: offsets[ix][<span class="number">1</span>]]</span><br><span class="line">        <span class="comment"># If the token is not the last token in the tweet, and the ending offset of the current token is less</span></span><br><span class="line">        <span class="comment"># than the beginning offset of the following token, add a space.</span></span><br><span class="line">        <span class="comment"># Basically, add a space when the next token (word piece) corresponds to a new word</span></span><br><span class="line">        <span class="keyword">if</span> (ix+<span class="number">1</span>) &lt; len(offsets) <span class="keyword">and</span> offsets[ix][<span class="number">1</span>] &lt; offsets[ix+<span class="number">1</span>][<span class="number">0</span>]:</span><br><span class="line">            filtered_output += <span class="string">" "</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the predicted output as the original tweet when the tweet's sentiment is "neutral", or the tweet only contains one word</span></span><br><span class="line">    <span class="keyword">if</span> sentiment_val == <span class="string">"neutral"</span> <span class="keyword">or</span> len(original_tweet.split()) &lt; <span class="number">2</span>:</span><br><span class="line">        filtered_output = original_tweet</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the jaccard score between the predicted span, and the actual span</span></span><br><span class="line">    <span class="comment"># The IOU (intersection over union) approach is detailed in the utils module's `jaccard` function:</span></span><br><span class="line">    <span class="comment"># https://www.kaggle.com/abhishek/utils</span></span><br><span class="line">    jac = utils.jaccard(target_string.strip(), filtered_output.strip())</span><br><span class="line">    <span class="keyword">return</span> jac, filtered_output</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval_fn</span><span class="params">(data_loader, model, device)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Evaluation function to predict on the test set</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Set model to evaluation mode</span></span><br><span class="line">    <span class="comment"># I.e., turn off dropout and set batchnorm to use overall mean and variance (from training), rather than batch level mean and variance</span></span><br><span class="line">    <span class="comment"># Reference: https://github.com/pytorch/pytorch/issues/5406</span></span><br><span class="line">    model.eval()</span><br><span class="line">    losses = utils.AverageMeter()</span><br><span class="line">    jaccards = utils.AverageMeter()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Turns off gradient calculations (https://datascience.stackexchange.com/questions/32651/what-is-the-use-of-torch-no-grad-in-pytorch)</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">        <span class="comment"># Make predictions and calculate loss / jaccard score for each batch</span></span><br><span class="line">        <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line">            ids = d[<span class="string">"ids"</span>]</span><br><span class="line">            token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">            mask = d[<span class="string">"mask"</span>]</span><br><span class="line">            sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">            orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">            orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">            targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">            targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">            offsets = d[<span class="string">"offsets"</span>].numpy()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Move tensors to GPU for faster matrix calculations</span></span><br><span class="line">            ids = ids.to(device, dtype=torch.long)</span><br><span class="line">            token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">            mask = mask.to(device, dtype=torch.long)</span><br><span class="line">            targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">            targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Predict logits for start and end indexes</span></span><br><span class="line">            outputs_start, outputs_end = model(</span><br><span class="line">                ids=ids,</span><br><span class="line">                mask=mask,</span><br><span class="line">                token_type_ids=token_type_ids</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># Calculate loss for the batch</span></span><br><span class="line">            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)</span><br><span class="line">            <span class="comment"># Apply softmax to the predicted logits for the start and end indexes</span></span><br><span class="line">            <span class="comment"># This converts the "logits" to "probability-like" scores</span></span><br><span class="line">            outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">            outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">            <span class="comment"># Calculate jaccard scores for each tweet in the batch</span></span><br><span class="line">            jaccard_scores = []</span><br><span class="line">            <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">                selected_tweet = orig_selected[px]</span><br><span class="line">                tweet_sentiment = sentiment[px]</span><br><span class="line">                jaccard_score, _ = calculate_jaccard_score(</span><br><span class="line">                    original_tweet=tweet,</span><br><span class="line">                    target_string=selected_tweet,</span><br><span class="line">                    sentiment_val=tweet_sentiment,</span><br><span class="line">                    idx_start=np.argmax(outputs_start[px, :]),</span><br><span class="line">                    idx_end=np.argmax(outputs_end[px, :]),</span><br><span class="line">                    offsets=offsets[px]</span><br><span class="line">                )</span><br><span class="line">                jaccard_scores.append(jaccard_score)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update running jaccard score and loss</span></span><br><span class="line">            jaccards.update(np.mean(jaccard_scores), ids.size(<span class="number">0</span>))</span><br><span class="line">            losses.update(loss.item(), ids.size(<span class="number">0</span>))</span><br><span class="line">            <span class="comment"># Print the running average loss and jaccard score</span></span><br><span class="line">            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f"Jaccard = <span class="subst">&#123;jaccards.avg&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">return</span> jaccards.avg</span><br></pre></td></tr></table></figure>
<h1 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(fold)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train model for a speciied fold</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Read training csv</span></span><br><span class="line">    dfx = pd.read_csv(config.TRAINING_FILE)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set train validation set split</span></span><br><span class="line">    df_train = dfx[dfx.kfold != fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_valid = dfx[dfx.kfold == fold].reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Instantiate TweetDataset with training data</span></span><br><span class="line">    train_dataset = TweetDataset(</span><br><span class="line">        tweet=df_train.text.values,</span><br><span class="line">        sentiment=df_train.sentiment.values,</span><br><span class="line">        selected_text=df_train.selected_text.values</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate DataLoader with `train_dataset`</span></span><br><span class="line">    <span class="comment"># This is a generator that yields the dataset in batches</span></span><br><span class="line">    train_data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        train_dataset,</span><br><span class="line">        batch_size=config.TRAIN_BATCH_SIZE,</span><br><span class="line">        num_workers=<span class="number">4</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate TweetDataset with validation data</span></span><br><span class="line">    valid_dataset = TweetDataset(</span><br><span class="line">        tweet=df_valid.text.values,</span><br><span class="line">        sentiment=df_valid.sentiment.values,</span><br><span class="line">        selected_text=df_valid.selected_text.values</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Instantiate DataLoader with `valid_dataset`</span></span><br><span class="line">    valid_data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        valid_dataset,</span><br><span class="line">        batch_size=config.VALID_BATCH_SIZE,</span><br><span class="line">        num_workers=<span class="number">2</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set device as `cuda` (GPU)</span></span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">    <span class="comment"># Load pretrained BERT (bert-base-uncased)</span></span><br><span class="line">    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)</span><br><span class="line">    <span class="comment"># Output hidden states</span></span><br><span class="line">    <span class="comment"># This is important to set since we want to concatenate the hidden states from the last 2 BERT layers</span></span><br><span class="line">    model_config.output_hidden_states = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># Instantiate our model with `model_config`</span></span><br><span class="line">    model = TweetModel(conf=model_config)</span><br><span class="line">    <span class="comment"># Move the model to the GPU</span></span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the number of training steps</span></span><br><span class="line">    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)</span><br><span class="line">    <span class="comment"># Get the list of named parameters</span></span><br><span class="line">    param_optimizer = list(model.named_parameters())</span><br><span class="line">    <span class="comment"># Specify parameters where weight decay shouldn't be applied</span></span><br><span class="line">    no_decay = [<span class="string">"bias"</span>, <span class="string">"LayerNorm.bias"</span>, <span class="string">"LayerNorm.weight"</span>]</span><br><span class="line">    <span class="comment"># Define two sets of parameters: those with weight decay, and those without</span></span><br><span class="line">    optimizer_parameters = [</span><br><span class="line">        &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> <span class="keyword">not</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.001</span>&#125;,</span><br><span class="line">        &#123;<span class="string">'params'</span>: [p <span class="keyword">for</span> n, p <span class="keyword">in</span> param_optimizer <span class="keyword">if</span> any(nd <span class="keyword">in</span> n <span class="keyword">for</span> nd <span class="keyword">in</span> no_decay)], <span class="string">'weight_decay'</span>: <span class="number">0.0</span>&#125;,</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment"># Instantiate AdamW optimizer with our two sets of parameters, and a learning rate of 3e-5</span></span><br><span class="line">    optimizer = AdamW(optimizer_parameters, lr=<span class="number">3e-5</span>)</span><br><span class="line">    <span class="comment"># Create a scheduler to set the learning rate at each training step</span></span><br><span class="line">    <span class="comment"># "Create a schedule with a learning rate that decreases linearly after linearly increasing during a warmup period." (https://pytorch.org/docs/stable/optim.html)</span></span><br><span class="line">    <span class="comment"># Since num_warmup_steps = 0, the learning rate starts at 3e-5, and then linearly decreases at each training step</span></span><br><span class="line">    scheduler = get_linear_schedule_with_warmup(</span><br><span class="line">        optimizer, </span><br><span class="line">        num_warmup_steps=<span class="number">0</span>, </span><br><span class="line">        num_training_steps=num_train_steps</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply early stopping with patience of 2</span></span><br><span class="line">    <span class="comment"># This means to stop training new epochs when 2 rounds have passed without any improvement</span></span><br><span class="line">    es = utils.EarlyStopping(patience=<span class="number">2</span>, mode=<span class="string">"max"</span>)</span><br><span class="line">    print(<span class="string">f"Training is Starting for fold=<span class="subst">&#123;fold&#125;</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># I'm training only for 3 epochs even though I specified 5!!!</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)</span><br><span class="line">        jaccard = eval_fn(valid_data_loader, model, device)</span><br><span class="line">        print(<span class="string">f"Jaccard Score = <span class="subst">&#123;jaccard&#125;</span>"</span>)</span><br><span class="line">        es(jaccard, model, model_path=<span class="string">f"model_<span class="subst">&#123;fold&#125;</span>.bin"</span>)</span><br><span class="line">        <span class="keyword">if</span> es.early_stop:</span><br><span class="line">            print(<span class="string">"Early stopping"</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">run(fold=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="Do-the-evaluation-on-test-data"><a href="#Do-the-evaluation-on-test-data" class="headerlink" title="Do the evaluation on test data"></a>Do the evaluation on test data</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_test = pd.read_csv(<span class="string">"../input/tweet-sentiment-extraction/test.csv"</span>)</span><br><span class="line">df_test.loc[:, <span class="string">"selected_text"</span>] = df_test.text.values</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_test</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>textID</th>
      <th>text</th>
      <th>sentiment</th>
      <th>selected_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>f87dea47db</td>
      <td>Last session of the day  http://twitpic.com/67ezh</td>
      <td>neutral</td>
      <td>Last session of the day  http://twitpic.com/67ezh</td>
    </tr>
    <tr>
      <th>1</th>
      <td>96d74cb729</td>
      <td>Shanghai is also really exciting (precisely -...</td>
      <td>positive</td>
      <td>Shanghai is also really exciting (precisely -...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>eee518ae67</td>
      <td>Recession hit Veronique Branquinho, she has to...</td>
      <td>negative</td>
      <td>Recession hit Veronique Branquinho, she has to...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01082688c6</td>
      <td>happy bday!</td>
      <td>positive</td>
      <td>happy bday!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33987a8ee5</td>
      <td>http://twitpic.com/4w75p - I like it!!</td>
      <td>positive</td>
      <td>http://twitpic.com/4w75p - I like it!!</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3529</th>
      <td>e5f0e6ef4b</td>
      <td>its at 3 am, im very tired but i can`t sleep  ...</td>
      <td>negative</td>
      <td>its at 3 am, im very tired but i can`t sleep  ...</td>
    </tr>
    <tr>
      <th>3530</th>
      <td>416863ce47</td>
      <td>All alone in this old house again.  Thanks for...</td>
      <td>positive</td>
      <td>All alone in this old house again.  Thanks for...</td>
    </tr>
    <tr>
      <th>3531</th>
      <td>6332da480c</td>
      <td>I know what you mean. My little dog is sinkin...</td>
      <td>negative</td>
      <td>I know what you mean. My little dog is sinkin...</td>
    </tr>
    <tr>
      <th>3532</th>
      <td>df1baec676</td>
      <td>_sutra what is your next youtube video gonna b...</td>
      <td>positive</td>
      <td>_sutra what is your next youtube video gonna b...</td>
    </tr>
    <tr>
      <th>3533</th>
      <td>469e15c5a8</td>
      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>
      <td>positive</td>
      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>
    </tr>
  </tbody>
</table>
<p>3534 rows × 4 columns</p>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line">model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)</span><br><span class="line">model_config.output_hidden_states = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Load each of the five trained models and move to GPU</span></span><br><span class="line">model1 = TweetModel(conf=model_config)</span><br><span class="line">model1.to(device)</span><br><span class="line">model1.load_state_dict(torch.load(<span class="string">"model_0.bin"</span>))</span><br><span class="line">model1.eval()</span><br><span class="line"></span><br><span class="line">model2 = TweetModel(conf=model_config)</span><br><span class="line">model2.to(device)</span><br><span class="line">model2.load_state_dict(torch.load(<span class="string">"model_1.bin"</span>))</span><br><span class="line">model2.eval()</span><br><span class="line"></span><br><span class="line">model3 = TweetModel(conf=model_config)</span><br><span class="line">model3.to(device)</span><br><span class="line">model3.load_state_dict(torch.load(<span class="string">"model_2.bin"</span>))</span><br><span class="line">model3.eval()</span><br><span class="line"></span><br><span class="line">model4 = TweetModel(conf=model_config)</span><br><span class="line">model4.to(device)</span><br><span class="line">model4.load_state_dict(torch.load(<span class="string">"model_3.bin"</span>))</span><br><span class="line">model4.eval()</span><br><span class="line"></span><br><span class="line">model5 = TweetModel(conf=model_config)</span><br><span class="line">model5.to(device)</span><br><span class="line">model5.load_state_dict(torch.load(<span class="string">"model_4.bin"</span>))</span><br><span class="line">model5.eval()</span><br></pre></td></tr></table></figure>
<pre><code>TweetModel(
  (roberta): RobertaModel(
    (embeddings): RobertaEmbeddings(
      (word_embeddings): Embedding(50265, 768, padding_idx=1)
      (position_embeddings): Embedding(514, 768, padding_idx=1)
      (token_type_embeddings): Embedding(1, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (drop_out): Dropout(p=0.1, inplace=False)
  (l_1): Linear(in_features=1536, out_features=400, bias=True)
  (l0): Linear(in_features=400, out_features=2, bias=True)
)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">final_output = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate TweetDataset with the test data</span></span><br><span class="line">test_dataset = TweetDataset(</span><br><span class="line">        tweet=df_test.text.values,</span><br><span class="line">        sentiment=df_test.sentiment.values,</span><br><span class="line">        selected_text=df_test.selected_text.values</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate DataLoader with `test_dataset`</span></span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line">    test_dataset,</span><br><span class="line">    shuffle=<span class="literal">False</span>,</span><br><span class="line">    batch_size=config.VALID_BATCH_SIZE,</span><br><span class="line">    num_workers=<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Turn of gradient calculations</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    tk0 = tqdm(data_loader, total=len(data_loader))</span><br><span class="line">    <span class="comment"># Predict the span containing the sentiment for each batch</span></span><br><span class="line">    <span class="keyword">for</span> bi, d <span class="keyword">in</span> enumerate(tk0):</span><br><span class="line">        ids = d[<span class="string">"ids"</span>]</span><br><span class="line">        token_type_ids = d[<span class="string">"token_type_ids"</span>]</span><br><span class="line">        mask = d[<span class="string">"mask"</span>]</span><br><span class="line">        sentiment = d[<span class="string">"sentiment"</span>]</span><br><span class="line">        orig_selected = d[<span class="string">"orig_selected"</span>]</span><br><span class="line">        orig_tweet = d[<span class="string">"orig_tweet"</span>]</span><br><span class="line">        targets_start = d[<span class="string">"targets_start"</span>]</span><br><span class="line">        targets_end = d[<span class="string">"targets_end"</span>]</span><br><span class="line">        offsets = d[<span class="string">"offsets"</span>].numpy()</span><br><span class="line"></span><br><span class="line">        ids = ids.to(device, dtype=torch.long)</span><br><span class="line">        token_type_ids = token_type_ids.to(device, dtype=torch.long)</span><br><span class="line">        mask = mask.to(device, dtype=torch.long)</span><br><span class="line">        targets_start = targets_start.to(device, dtype=torch.long)</span><br><span class="line">        targets_end = targets_end.to(device, dtype=torch.long)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Predict start and end logits for each of the five models</span></span><br><span class="line">        outputs_start1, outputs_end1 = model1(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start2, outputs_end2 = model2(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start3, outputs_end3 = model3(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start4, outputs_end4 = model4(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        outputs_start5, outputs_end5 = model5(</span><br><span class="line">            ids=ids,</span><br><span class="line">            mask=mask,</span><br><span class="line">            token_type_ids=token_type_ids</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Get the average start and end logits across the five models and use these as predictions</span></span><br><span class="line">        <span class="comment"># This is a form of "ensembling"</span></span><br><span class="line">        outputs_start = (</span><br><span class="line">            outputs_start1 </span><br><span class="line">            + outputs_start2 </span><br><span class="line">            + outputs_start3 </span><br><span class="line">            + outputs_start4 </span><br><span class="line">            + outputs_start5</span><br><span class="line">        ) / <span class="number">5</span></span><br><span class="line">        outputs_end = (</span><br><span class="line">            outputs_end1 </span><br><span class="line">            + outputs_end2 </span><br><span class="line">            + outputs_end3 </span><br><span class="line">            + outputs_end4 </span><br><span class="line">            + outputs_end5</span><br><span class="line">        ) / <span class="number">5</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Apply softmax to the predicted start and end logits</span></span><br><span class="line">        outputs_start = torch.softmax(outputs_start, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line">        outputs_end = torch.softmax(outputs_end, dim=<span class="number">1</span>).cpu().detach().numpy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert the start and end scores to actual predicted spans (in string form)</span></span><br><span class="line">        <span class="keyword">for</span> px, tweet <span class="keyword">in</span> enumerate(orig_tweet):</span><br><span class="line">            selected_tweet = orig_selected[px]</span><br><span class="line">            tweet_sentiment = sentiment[px]</span><br><span class="line">            _, output_sentence = calculate_jaccard_score(</span><br><span class="line">                original_tweet=tweet,</span><br><span class="line">                target_string=selected_tweet,</span><br><span class="line">                sentiment_val=tweet_sentiment,</span><br><span class="line">                idx_start=np.argmax(outputs_start[px, :]),</span><br><span class="line">                idx_end=np.argmax(outputs_end[px, :]),</span><br><span class="line">                offsets=offsets[px]</span><br><span class="line">            )</span><br><span class="line">            final_output.append(output_sentence)</span><br></pre></td></tr></table></figure>
<pre><code>HBox(children=(FloatProgress(value=0.0, max=221.0), HTML(value=&#39;&#39;)))
</code></pre><p>​<br>​    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># post-process trick:</span></span><br><span class="line"><span class="comment"># Note: This trick comes from: https://www.kaggle.com/c/tweet-sentiment-extraction/discussion/140942</span></span><br><span class="line"><span class="comment"># When the LB resets, this trick won't help</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post_process</span><span class="params">(selected)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">" "</span>.join(set(selected.lower().split()))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sample = pd.read_csv(<span class="string">"../input/tweet-sentiment-extraction/sample_submission.csv"</span>)</span><br><span class="line">sample.loc[:, <span class="string">'selected_text'</span>] = final_output</span><br><span class="line">sample.selected_text = sample.selected_text.map(post_process)</span><br><span class="line">sample.to_csv(<span class="string">"submission.csv"</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample.head()</span><br></pre></td></tr></table></figure>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }

</style>

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>textID</th>
      <th>selected_text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>f87dea47db</td>
      <td>http://twitpic.com/67ezh the of day session last</td>
    </tr>
    <tr>
      <th>1</th>
      <td>96d74cb729</td>
      <td>exciting</td>
    </tr>
    <tr>
      <th>2</th>
      <td>eee518ae67</td>
      <td>such shame! a</td>
    </tr>
    <tr>
      <th>3</th>
      <td>01082688c6</td>
      <td>happy bday!</td>
    </tr>
    <tr>
      <th>4</th>
      <td>33987a8ee5</td>
      <td>i like</td>
    </tr>
  </tbody>
</table>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line">source_DF = inspect.getsource(utils)</span><br><span class="line">print(source_DF)</span><br></pre></td></tr></table></figure>
<pre><code>import numpy as np
import torch
</code></pre><p>​    </p>
<pre><code>class AverageMeter:
    &quot;&quot;&quot;
    Computes and stores the average and current value
    &quot;&quot;&quot;
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count
</code></pre><p>​    </p>
<pre><code>class EarlyStopping:
    def __init__(self, patience=7, mode=&quot;max&quot;, delta=0.001):
        self.patience = patience
        self.counter = 0
        self.mode = mode
        self.best_score = None
        self.early_stop = False
        self.delta = delta
        if self.mode == &quot;min&quot;:
            self.val_score = np.Inf
        else:
            self.val_score = -np.Inf

    def __call__(self, epoch_score, model, model_path):

        if self.mode == &quot;min&quot;:
            score = -1.0 * epoch_score
        else:
            score = np.copy(epoch_score)

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
        elif score &lt; self.best_score + self.delta:
            self.counter += 1
            print(&#39;EarlyStopping counter: {} out of {}&#39;.format(self.counter, self.patience))
            if self.counter &gt;= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(epoch_score, model, model_path)
            self.counter = 0

    def save_checkpoint(self, epoch_score, model, model_path):
        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:
            print(&#39;Validation score improved ({} --&gt; {}). Saving model!&#39;.format(self.val_score, epoch_score))
            torch.save(model.state_dict(), model_path)
        self.val_score = epoch_score
</code></pre><p>​    </p>
<pre><code>def jaccard(str1, str2): 
    a = set(str1.lower().split()) 
    b = set(str2.lower().split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))
</code></pre><p>​    </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/24/%E6%99%AE%E9%80%9A%E5%AD%A6%E7%94%9F%EF%BC%8C%E5%A6%82%E4%BD%95%E7%94%A8Colab%E8%B7%91%E8%B5%B7%E6%9D%A5BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/24/%E6%99%AE%E9%80%9A%E5%AD%A6%E7%94%9F%EF%BC%8C%E5%A6%82%E4%BD%95%E7%94%A8Colab%E8%B7%91%E8%B5%B7%E6%9D%A5BERT%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%EF%BC%9F/" class="post-title-link" itemprop="url">普通学生，如何用Colab跑起来BERT系列模型？</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-24 23:04:11 / 修改时间：23:04:48" itemprop="dateCreated datePublished" datetime="2020-04-24T23:04:11+08:00">2020-04-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index"><span itemprop="name">nlp</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dxzmpk.github.io/2020/04/22/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B7%A5%E5%85%B7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="董雄">
      <meta itemprop="description" content="I am a fresh researcher in nlp in Harbin Institute of Technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="董雄的博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/22/%E7%AE%97%E6%B3%95%E8%AE%BE%E8%AE%A1%E5%9F%BA%E7%A1%80-%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E4%B8%A4%E4%B8%AA%E5%B7%A5%E5%85%B7/" class="post-title-link" itemprop="url">算法设计基础-最重要的两个工具</a>
        </h2>

        <div class="post-meta">
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-22 15:02:12" itemprop="dateCreated datePublished" datetime="2020-04-22T15:02:12+08:00">2020-04-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-23 09:00:46" itemprop="dateModified" datetime="2020-04-23T09:00:46+08:00">2020-04-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/algorithms/" itemprop="url" rel="index"><span itemprop="name">algorithms</span></a>
                </span>
            </span>

          
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="两个基础"><a href="#两个基础" class="headerlink" title="两个基础"></a>两个基础</h3><ol>
<li><p>算法设计中的一项重要的技术是缩小允许的实例的集合，直到找到正确有效的算法为止。</p>
</li>
<li><p>要利用好已有的算法，您必须学会基本过程中的“抽象地”描述问题。根据定义好的结构和算法对应用程序进行建模是实现解决方案的最重要的一步。</p>
</li>
</ol>
<h3 id="RAM-和-大O"><a href="#RAM-和-大O" class="headerlink" title="RAM 和 大O"></a>RAM 和 大O</h3><ol>
<li><p>RAM计算模型</p>
<p>将机器抽象为一个简单的机器，在这个机器上进行计算：</p>
<ul>
<li>每个简单的操作（+，*，-，=，if，call）只需一个时间步。</li>
<li>循环和子例程不被视为简单操作</li>
<li>每次内存访问仅需一个时间步长，而无需注意缓存或磁盘中的内容</li>
</ul>
</li>
<li><p>大O</p>
<p>这些复杂性中的每一个都定义了一个数字函数，表示时间与问题大小的关系。但是时间上的复杂性是如此复杂，以至于我们必须简化它们以实现利用。为此，我们需要使用大O符号</p>
<p>首先，要精确计算复杂度非常困难，因为它倾向于：</p>
</li>
</ol>
<ul>
<li><p>变化很大，不稳定</p>
</li>
<li><p>需要太多细节才能精确指定。因为计算最坏情况下执行的RAM指令的确切数量需要把整个计算机程序的细节考虑进来。</p>
<p> 大O符号会精确到不影响我们算法比较的详细程度。</p>
<h3 id="一个原则"><a href="#一个原则" class="headerlink" title="一个原则"></a>一个原则</h3></li>
<li><p>最重要的原则：<strong>Big Oh符号</strong>和<strong>最坏情况分析</strong>是极大简化我们比较算法效率的功能的工具。</p>
</li>
<li><p>大O的通俗解法：找一个c，不管n怎么变大，$g(n)$都是其上界。注意，$g(n)$是比较大的那一个</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">董雄</p>
  <div class="site-description" itemprop="description">I am a fresh researcher in nlp in Harbin Institute of Technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">董雄</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        


  <div style="display: none;">
    <script src="//s95.cnzz.com/z_stat.php?id=1278837449&web_id=1278837449"></script>
  </div>






      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 't8n8RTilca5Gm0vKToaMrjRU-gzGzoHsz',
      appKey     : 'x4Jty8MrDpczjPANtbbGhwXX',
      placeholder: "请留下一点痕迹吧, 评论将永远留存",
      avatar     : 'robohash',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
